{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from trading_session_gym.envs.trading_session_gym import TradingSession\n",
    "from train import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingSession(action_space_config = 'discrete')\n",
    "\n",
    "net = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "net.load_state_dict(torch.load('model.dat', map_location=lambda storage, loc: storage))\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "rewards = np.array(env.get_reward())\n",
    "prices = np.array(env.get_prices())\n",
    "holdings_quantity = np.array(env.get_holdings_quantity())\n",
    "\n",
    "done = False\n",
    "\n",
    "while done == False:\n",
    "    state_v = torch.tensor(np.array([state], copy=False))\n",
    "    q_vals = net(state_v).data.numpy()[0]\n",
    "    action = np.argmax(q_vals)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "\n",
    "    rewards = np.append(rewards, reward)\n",
    "    prices = np.vstack([prices, env.get_prices()])\n",
    "    holdings_quantity = np.vstack([holdings_quantity, env.get_holdings_quantity()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, figsize = [15,15], sharex = True)\n",
    "\n",
    "axs[0].plot(prices)\n",
    "axs[0].set_ylabel('Prices')\n",
    "\n",
    "axs[1].plot(holdings_quantity)\n",
    "axs[1].set_ylabel('Holdings')\n",
    "\n",
    "axs[2].plot(rewards)\n",
    "axs[2].set_ylabel('Reward')\n",
    "\n",
    "axs[0].set_xlim([0, len(prices)])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_strategy_reward = 1000*env.get_boundary()/(np.min(prices))\n",
    "naive_strategy_reward = np.add.accumulate(rewards)[-1]\n",
    "naive_performance = naive_strategy_reward/optimal_strategy_reward\n",
    "\n",
    "print(\"Performance: {}%\". format(round(100*naive_performance, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_strategy_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_strategy_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
