{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from trading_session_gym.envs.trading_session_gym import TradingSession\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones), np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-network with target network\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        # network\n",
    "        self.fc = nn.Sequential(\n",
    "                    nn.Linear(n_inputs, n_inputs),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_inputs, n_outputs)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "        \n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(batch, net, tgt_net, device=\"cpu\", cuda_async=False):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.ByteTensor(dones).to(device)\n",
    "      \n",
    "    if device==\"cuda\":\n",
    "        states_v = states_v.cuda(non_blocking=cuda_async)\n",
    "        next_states_v = next_states_v.cuda(non_blocking=cuda_async)\n",
    "        actions_v = actions_v.cuda(non_blocking=cuda_async)\n",
    "        rewards_v = rewards_v.cuda(non_blocking=cuda_async)\n",
    "        done_mask = done_mask.cuda(non_blocking=cuda_async)\n",
    "        \n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1).long()).squeeze(-1)\n",
    "    \n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "    next_state_values[done_mask] = 0.0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_REWARD_BOUND = 70\n",
    "\n",
    "GAMMA = 0\n",
    "BATCH_SIZE = 100\n",
    "REPLAY_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_STEPS = 1000\n",
    "REPLAY_START_SIZE = 10000\n",
    "\n",
    "EPSILON_DECAY = 10**5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=24, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=24, out_features=13, bias=True)\n",
      "  )\n",
      ")\n",
      "1440: done 1 episodes, mean reward 56.523, eps 0.99\n",
      "2880: done 2 episodes, mean reward 57.950, eps 0.97\n",
      "Best mean reward updated 56.523 -> 57.950, model saved\n",
      "4320: done 3 episodes, mean reward 57.824, eps 0.96\n",
      "5760: done 4 episodes, mean reward 58.213, eps 0.94\n",
      "Best mean reward updated 57.950 -> 58.213, model saved\n",
      "7200: done 5 episodes, mean reward 57.831, eps 0.93\n",
      "8640: done 6 episodes, mean reward 57.859, eps 0.91\n",
      "10080: done 7 episodes, mean reward 57.701, eps 0.90\n",
      "11520: done 8 episodes, mean reward 57.564, eps 0.88\n",
      "12960: done 9 episodes, mean reward 58.051, eps 0.87\n",
      "14400: done 10 episodes, mean reward 58.231, eps 0.86\n",
      "Best mean reward updated 58.213 -> 58.231, model saved\n",
      "15840: done 11 episodes, mean reward 58.267, eps 0.84\n",
      "Best mean reward updated 58.231 -> 58.267, model saved\n",
      "17280: done 12 episodes, mean reward 58.647, eps 0.83\n",
      "Best mean reward updated 58.267 -> 58.647, model saved\n",
      "18720: done 13 episodes, mean reward 58.565, eps 0.81\n",
      "20160: done 14 episodes, mean reward 58.609, eps 0.80\n",
      "21600: done 15 episodes, mean reward 58.732, eps 0.78\n",
      "Best mean reward updated 58.647 -> 58.732, model saved\n",
      "23040: done 16 episodes, mean reward 58.516, eps 0.77\n",
      "24480: done 17 episodes, mean reward 58.312, eps 0.76\n",
      "25920: done 18 episodes, mean reward 58.784, eps 0.74\n",
      "Best mean reward updated 58.732 -> 58.784, model saved\n",
      "27360: done 19 episodes, mean reward 59.023, eps 0.73\n",
      "Best mean reward updated 58.784 -> 59.023, model saved\n",
      "28800: done 20 episodes, mean reward 58.937, eps 0.71\n",
      "30240: done 21 episodes, mean reward 58.758, eps 0.70\n",
      "31680: done 22 episodes, mean reward 58.739, eps 0.68\n",
      "33120: done 23 episodes, mean reward 58.662, eps 0.67\n",
      "34560: done 24 episodes, mean reward 58.516, eps 0.65\n",
      "36000: done 25 episodes, mean reward 58.488, eps 0.64\n",
      "37440: done 26 episodes, mean reward 58.479, eps 0.63\n",
      "38880: done 27 episodes, mean reward 58.501, eps 0.61\n",
      "40320: done 28 episodes, mean reward 58.408, eps 0.60\n",
      "41760: done 29 episodes, mean reward 58.431, eps 0.58\n",
      "43200: done 30 episodes, mean reward 58.396, eps 0.57\n",
      "44640: done 31 episodes, mean reward 58.357, eps 0.55\n",
      "46080: done 32 episodes, mean reward 58.241, eps 0.54\n",
      "47520: done 33 episodes, mean reward 58.275, eps 0.52\n",
      "48960: done 34 episodes, mean reward 58.238, eps 0.51\n",
      "50400: done 35 episodes, mean reward 58.149, eps 0.50\n",
      "51840: done 36 episodes, mean reward 58.398, eps 0.48\n",
      "53280: done 37 episodes, mean reward 58.423, eps 0.47\n",
      "54720: done 38 episodes, mean reward 58.468, eps 0.45\n",
      "56160: done 39 episodes, mean reward 58.472, eps 0.44\n",
      "57600: done 40 episodes, mean reward 58.417, eps 0.42\n",
      "59040: done 41 episodes, mean reward 58.338, eps 0.41\n",
      "60480: done 42 episodes, mean reward 58.341, eps 0.40\n",
      "61920: done 43 episodes, mean reward 58.378, eps 0.38\n",
      "63360: done 44 episodes, mean reward 58.419, eps 0.37\n",
      "64800: done 45 episodes, mean reward 58.367, eps 0.35\n",
      "66240: done 46 episodes, mean reward 58.377, eps 0.34\n",
      "67680: done 47 episodes, mean reward 58.347, eps 0.32\n",
      "69120: done 48 episodes, mean reward 58.287, eps 0.31\n",
      "70560: done 49 episodes, mean reward 58.215, eps 0.29\n",
      "72000: done 50 episodes, mean reward 58.150, eps 0.28\n",
      "73440: done 51 episodes, mean reward 58.112, eps 0.27\n",
      "74880: done 52 episodes, mean reward 58.039, eps 0.25\n",
      "76320: done 53 episodes, mean reward 57.972, eps 0.24\n",
      "77760: done 54 episodes, mean reward 57.894, eps 0.22\n",
      "79200: done 55 episodes, mean reward 57.869, eps 0.21\n",
      "80640: done 56 episodes, mean reward 57.917, eps 0.19\n",
      "82080: done 57 episodes, mean reward 57.873, eps 0.18\n",
      "83520: done 58 episodes, mean reward 57.828, eps 0.16\n",
      "84960: done 59 episodes, mean reward 57.782, eps 0.15\n",
      "86400: done 60 episodes, mean reward 57.724, eps 0.14\n",
      "87840: done 61 episodes, mean reward 57.684, eps 0.12\n",
      "89280: done 62 episodes, mean reward 57.615, eps 0.11\n",
      "90720: done 63 episodes, mean reward 57.546, eps 0.09\n",
      "92160: done 64 episodes, mean reward 57.495, eps 0.08\n",
      "93600: done 65 episodes, mean reward 57.404, eps 0.06\n",
      "95040: done 66 episodes, mean reward 57.329, eps 0.05\n",
      "96480: done 67 episodes, mean reward 57.271, eps 0.04\n",
      "97920: done 68 episodes, mean reward 57.226, eps 0.02\n",
      "99360: done 69 episodes, mean reward 57.238, eps 0.02\n",
      "100800: done 70 episodes, mean reward 57.271, eps 0.02\n",
      "102240: done 71 episodes, mean reward 57.188, eps 0.02\n",
      "103680: done 72 episodes, mean reward 57.152, eps 0.02\n",
      "105120: done 73 episodes, mean reward 57.113, eps 0.02\n",
      "106560: done 74 episodes, mean reward 57.117, eps 0.02\n",
      "108000: done 75 episodes, mean reward 57.045, eps 0.02\n",
      "109440: done 76 episodes, mean reward 57.011, eps 0.02\n",
      "110880: done 77 episodes, mean reward 57.017, eps 0.02\n",
      "112320: done 78 episodes, mean reward 57.030, eps 0.02\n",
      "113760: done 79 episodes, mean reward 57.035, eps 0.02\n",
      "115200: done 80 episodes, mean reward 57.035, eps 0.02\n",
      "116640: done 81 episodes, mean reward 57.025, eps 0.02\n",
      "118080: done 82 episodes, mean reward 57.002, eps 0.02\n",
      "119520: done 83 episodes, mean reward 56.946, eps 0.02\n",
      "120960: done 84 episodes, mean reward 56.950, eps 0.02\n",
      "122400: done 85 episodes, mean reward 56.921, eps 0.02\n",
      "123840: done 86 episodes, mean reward 56.878, eps 0.02\n",
      "125280: done 87 episodes, mean reward 56.919, eps 0.02\n",
      "126720: done 88 episodes, mean reward 56.916, eps 0.02\n",
      "128160: done 89 episodes, mean reward 56.895, eps 0.02\n",
      "129600: done 90 episodes, mean reward 56.891, eps 0.02\n",
      "131040: done 91 episodes, mean reward 56.904, eps 0.02\n",
      "132480: done 92 episodes, mean reward 56.878, eps 0.02\n",
      "133920: done 93 episodes, mean reward 56.848, eps 0.02\n",
      "135360: done 94 episodes, mean reward 56.803, eps 0.02\n",
      "136800: done 95 episodes, mean reward 56.782, eps 0.02\n",
      "138240: done 96 episodes, mean reward 56.749, eps 0.02\n",
      "139680: done 97 episodes, mean reward 56.750, eps 0.02\n",
      "141120: done 98 episodes, mean reward 56.711, eps 0.02\n",
      "142560: done 99 episodes, mean reward 56.683, eps 0.02\n",
      "144000: done 100 episodes, mean reward 56.625, eps 0.02\n",
      "145440: done 101 episodes, mean reward 56.623, eps 0.02\n",
      "146880: done 102 episodes, mean reward 56.587, eps 0.02\n",
      "148320: done 103 episodes, mean reward 56.552, eps 0.02\n",
      "149760: done 104 episodes, mean reward 56.547, eps 0.02\n",
      "151200: done 105 episodes, mean reward 56.530, eps 0.02\n",
      "152640: done 106 episodes, mean reward 56.507, eps 0.02\n",
      "154080: done 107 episodes, mean reward 56.451, eps 0.02\n",
      "155520: done 108 episodes, mean reward 56.539, eps 0.02\n",
      "156960: done 109 episodes, mean reward 56.485, eps 0.02\n",
      "158400: done 110 episodes, mean reward 56.421, eps 0.02\n",
      "159840: done 111 episodes, mean reward 56.391, eps 0.02\n",
      "161280: done 112 episodes, mean reward 56.306, eps 0.02\n",
      "162720: done 113 episodes, mean reward 56.292, eps 0.02\n",
      "164160: done 114 episodes, mean reward 56.248, eps 0.02\n",
      "165600: done 115 episodes, mean reward 56.152, eps 0.02\n",
      "167040: done 116 episodes, mean reward 56.135, eps 0.02\n",
      "168480: done 117 episodes, mean reward 56.113, eps 0.02\n",
      "169920: done 118 episodes, mean reward 56.011, eps 0.02\n",
      "171360: done 119 episodes, mean reward 55.938, eps 0.02\n",
      "172800: done 120 episodes, mean reward 55.970, eps 0.02\n",
      "174240: done 121 episodes, mean reward 55.950, eps 0.02\n",
      "175680: done 122 episodes, mean reward 55.950, eps 0.02\n",
      "177120: done 123 episodes, mean reward 55.950, eps 0.02\n",
      "178560: done 124 episodes, mean reward 55.964, eps 0.02\n",
      "180000: done 125 episodes, mean reward 55.965, eps 0.02\n",
      "181440: done 126 episodes, mean reward 55.890, eps 0.02\n",
      "182880: done 127 episodes, mean reward 55.826, eps 0.02\n",
      "184320: done 128 episodes, mean reward 55.821, eps 0.02\n",
      "185760: done 129 episodes, mean reward 55.770, eps 0.02\n",
      "187200: done 130 episodes, mean reward 55.777, eps 0.02\n",
      "188640: done 131 episodes, mean reward 55.759, eps 0.02\n",
      "190080: done 132 episodes, mean reward 55.790, eps 0.02\n",
      "191520: done 133 episodes, mean reward 55.741, eps 0.02\n",
      "192960: done 134 episodes, mean reward 55.730, eps 0.02\n",
      "194400: done 135 episodes, mean reward 55.741, eps 0.02\n",
      "195840: done 136 episodes, mean reward 55.630, eps 0.02\n",
      "197280: done 137 episodes, mean reward 55.600, eps 0.02\n",
      "198720: done 138 episodes, mean reward 55.529, eps 0.02\n",
      "200160: done 139 episodes, mean reward 55.475, eps 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201600: done 140 episodes, mean reward 55.447, eps 0.02\n",
      "203040: done 141 episodes, mean reward 55.448, eps 0.02\n",
      "204480: done 142 episodes, mean reward 55.428, eps 0.02\n",
      "205920: done 143 episodes, mean reward 55.372, eps 0.02\n",
      "207360: done 144 episodes, mean reward 55.296, eps 0.02\n",
      "208800: done 145 episodes, mean reward 55.277, eps 0.02\n",
      "210240: done 146 episodes, mean reward 55.238, eps 0.02\n",
      "211680: done 147 episodes, mean reward 55.241, eps 0.02\n",
      "213120: done 148 episodes, mean reward 55.226, eps 0.02\n",
      "214560: done 149 episodes, mean reward 55.268, eps 0.02\n",
      "216000: done 150 episodes, mean reward 55.238, eps 0.02\n",
      "217440: done 151 episodes, mean reward 55.238, eps 0.02\n",
      "218880: done 152 episodes, mean reward 55.253, eps 0.02\n",
      "220320: done 153 episodes, mean reward 55.249, eps 0.02\n",
      "221760: done 154 episodes, mean reward 55.276, eps 0.02\n",
      "223200: done 155 episodes, mean reward 55.266, eps 0.02\n",
      "224640: done 156 episodes, mean reward 55.251, eps 0.02\n",
      "226080: done 157 episodes, mean reward 55.253, eps 0.02\n",
      "227520: done 158 episodes, mean reward 55.251, eps 0.02\n",
      "228960: done 159 episodes, mean reward 55.242, eps 0.02\n",
      "230400: done 160 episodes, mean reward 55.273, eps 0.02\n",
      "231840: done 161 episodes, mean reward 55.265, eps 0.02\n",
      "233280: done 162 episodes, mean reward 55.297, eps 0.02\n",
      "234720: done 163 episodes, mean reward 55.311, eps 0.02\n",
      "236160: done 164 episodes, mean reward 55.317, eps 0.02\n",
      "237600: done 165 episodes, mean reward 55.338, eps 0.02\n",
      "239040: done 166 episodes, mean reward 55.369, eps 0.02\n",
      "240480: done 167 episodes, mean reward 55.379, eps 0.02\n",
      "241920: done 168 episodes, mean reward 55.379, eps 0.02\n",
      "243360: done 169 episodes, mean reward 55.340, eps 0.02\n",
      "244800: done 170 episodes, mean reward 55.280, eps 0.02\n",
      "246240: done 171 episodes, mean reward 55.310, eps 0.02\n",
      "247680: done 172 episodes, mean reward 55.317, eps 0.02\n",
      "249120: done 173 episodes, mean reward 55.320, eps 0.02\n",
      "250560: done 174 episodes, mean reward 55.322, eps 0.02\n",
      "252000: done 175 episodes, mean reward 55.336, eps 0.02\n",
      "253440: done 176 episodes, mean reward 55.353, eps 0.02\n",
      "254880: done 177 episodes, mean reward 55.362, eps 0.02\n",
      "256320: done 178 episodes, mean reward 55.332, eps 0.02\n",
      "257760: done 179 episodes, mean reward 55.336, eps 0.02\n",
      "259200: done 180 episodes, mean reward 55.338, eps 0.02\n",
      "260640: done 181 episodes, mean reward 55.342, eps 0.02\n",
      "262080: done 182 episodes, mean reward 55.394, eps 0.02\n",
      "263520: done 183 episodes, mean reward 55.430, eps 0.02\n",
      "264960: done 184 episodes, mean reward 55.404, eps 0.02\n",
      "266400: done 185 episodes, mean reward 55.416, eps 0.02\n",
      "267840: done 186 episodes, mean reward 55.435, eps 0.02\n",
      "269280: done 187 episodes, mean reward 55.367, eps 0.02\n",
      "270720: done 188 episodes, mean reward 55.376, eps 0.02\n",
      "272160: done 189 episodes, mean reward 55.391, eps 0.02\n",
      "273600: done 190 episodes, mean reward 55.359, eps 0.02\n",
      "275040: done 191 episodes, mean reward 55.340, eps 0.02\n",
      "276480: done 192 episodes, mean reward 55.312, eps 0.02\n",
      "277920: done 193 episodes, mean reward 55.343, eps 0.02\n",
      "279360: done 194 episodes, mean reward 55.372, eps 0.02\n",
      "280800: done 195 episodes, mean reward 55.362, eps 0.02\n",
      "282240: done 196 episodes, mean reward 55.386, eps 0.02\n",
      "283680: done 197 episodes, mean reward 55.372, eps 0.02\n",
      "285120: done 198 episodes, mean reward 55.371, eps 0.02\n",
      "286560: done 199 episodes, mean reward 55.374, eps 0.02\n",
      "288000: done 200 episodes, mean reward 55.428, eps 0.02\n",
      "289440: done 201 episodes, mean reward 55.431, eps 0.02\n",
      "290880: done 202 episodes, mean reward 55.422, eps 0.02\n",
      "292320: done 203 episodes, mean reward 55.426, eps 0.02\n",
      "293760: done 204 episodes, mean reward 55.391, eps 0.02\n",
      "295200: done 205 episodes, mean reward 55.379, eps 0.02\n",
      "296640: done 206 episodes, mean reward 55.384, eps 0.02\n",
      "298080: done 207 episodes, mean reward 55.431, eps 0.02\n",
      "299520: done 208 episodes, mean reward 55.424, eps 0.02\n",
      "300960: done 209 episodes, mean reward 55.424, eps 0.02\n",
      "302400: done 210 episodes, mean reward 55.506, eps 0.02\n",
      "303840: done 211 episodes, mean reward 55.514, eps 0.02\n",
      "305280: done 212 episodes, mean reward 55.531, eps 0.02\n",
      "306720: done 213 episodes, mean reward 55.495, eps 0.02\n",
      "308160: done 214 episodes, mean reward 55.522, eps 0.02\n",
      "309600: done 215 episodes, mean reward 55.551, eps 0.02\n",
      "311040: done 216 episodes, mean reward 55.534, eps 0.02\n",
      "312480: done 217 episodes, mean reward 55.542, eps 0.02\n",
      "313920: done 218 episodes, mean reward 55.557, eps 0.02\n",
      "315360: done 219 episodes, mean reward 55.539, eps 0.02\n",
      "316800: done 220 episodes, mean reward 55.474, eps 0.02\n",
      "318240: done 221 episodes, mean reward 55.481, eps 0.02\n",
      "319680: done 222 episodes, mean reward 55.450, eps 0.02\n",
      "321120: done 223 episodes, mean reward 55.441, eps 0.02\n",
      "322560: done 224 episodes, mean reward 55.425, eps 0.02\n",
      "324000: done 225 episodes, mean reward 55.394, eps 0.02\n",
      "325440: done 226 episodes, mean reward 55.475, eps 0.02\n",
      "326880: done 227 episodes, mean reward 55.505, eps 0.02\n",
      "328320: done 228 episodes, mean reward 55.489, eps 0.02\n",
      "329760: done 229 episodes, mean reward 55.502, eps 0.02\n",
      "331200: done 230 episodes, mean reward 55.469, eps 0.02\n",
      "332640: done 231 episodes, mean reward 55.491, eps 0.02\n",
      "334080: done 232 episodes, mean reward 55.501, eps 0.02\n",
      "335520: done 233 episodes, mean reward 55.495, eps 0.02\n",
      "336960: done 234 episodes, mean reward 55.533, eps 0.02\n",
      "338400: done 235 episodes, mean reward 55.527, eps 0.02\n",
      "339840: done 236 episodes, mean reward 55.505, eps 0.02\n",
      "341280: done 237 episodes, mean reward 55.483, eps 0.02\n",
      "342720: done 238 episodes, mean reward 55.529, eps 0.02\n",
      "344160: done 239 episodes, mean reward 55.585, eps 0.02\n",
      "345600: done 240 episodes, mean reward 55.614, eps 0.02\n",
      "347040: done 241 episodes, mean reward 55.623, eps 0.02\n",
      "348480: done 242 episodes, mean reward 55.641, eps 0.02\n",
      "349920: done 243 episodes, mean reward 55.662, eps 0.02\n",
      "351360: done 244 episodes, mean reward 55.703, eps 0.02\n",
      "352800: done 245 episodes, mean reward 55.764, eps 0.02\n",
      "354240: done 246 episodes, mean reward 55.777, eps 0.02\n",
      "355680: done 247 episodes, mean reward 55.751, eps 0.02\n",
      "357120: done 248 episodes, mean reward 55.852, eps 0.02\n",
      "358560: done 249 episodes, mean reward 55.792, eps 0.02\n",
      "360000: done 250 episodes, mean reward 55.839, eps 0.02\n",
      "361440: done 251 episodes, mean reward 55.847, eps 0.02\n",
      "362880: done 252 episodes, mean reward 55.848, eps 0.02\n",
      "364320: done 253 episodes, mean reward 55.897, eps 0.02\n",
      "365760: done 254 episodes, mean reward 55.867, eps 0.02\n",
      "367200: done 255 episodes, mean reward 55.860, eps 0.02\n",
      "368640: done 256 episodes, mean reward 55.829, eps 0.02\n",
      "370080: done 257 episodes, mean reward 55.831, eps 0.02\n",
      "371520: done 258 episodes, mean reward 55.806, eps 0.02\n",
      "372960: done 259 episodes, mean reward 55.818, eps 0.02\n",
      "374400: done 260 episodes, mean reward 55.781, eps 0.02\n",
      "375840: done 261 episodes, mean reward 55.763, eps 0.02\n",
      "377280: done 262 episodes, mean reward 55.810, eps 0.02\n",
      "378720: done 263 episodes, mean reward 55.846, eps 0.02\n",
      "380160: done 264 episodes, mean reward 55.834, eps 0.02\n",
      "381600: done 265 episodes, mean reward 55.891, eps 0.02\n",
      "383040: done 266 episodes, mean reward 56.004, eps 0.02\n",
      "384480: done 267 episodes, mean reward 56.001, eps 0.02\n",
      "385920: done 268 episodes, mean reward 56.091, eps 0.02\n",
      "387360: done 269 episodes, mean reward 56.148, eps 0.02\n",
      "388800: done 270 episodes, mean reward 56.411, eps 0.02\n",
      "390240: done 271 episodes, mean reward 56.463, eps 0.02\n",
      "391680: done 272 episodes, mean reward 56.473, eps 0.02\n",
      "393120: done 273 episodes, mean reward 56.470, eps 0.02\n",
      "394560: done 274 episodes, mean reward 56.458, eps 0.02\n",
      "396000: done 275 episodes, mean reward 56.490, eps 0.02\n",
      "397440: done 276 episodes, mean reward 56.513, eps 0.02\n",
      "398880: done 277 episodes, mean reward 56.540, eps 0.02\n",
      "400320: done 278 episodes, mean reward 56.645, eps 0.02\n",
      "401760: done 279 episodes, mean reward 56.720, eps 0.02\n",
      "403200: done 280 episodes, mean reward 56.744, eps 0.02\n",
      "404640: done 281 episodes, mean reward 56.758, eps 0.02\n",
      "406080: done 282 episodes, mean reward 56.760, eps 0.02\n",
      "407520: done 283 episodes, mean reward 56.789, eps 0.02\n",
      "408960: done 284 episodes, mean reward 56.880, eps 0.02\n",
      "410400: done 285 episodes, mean reward 56.942, eps 0.02\n",
      "411840: done 286 episodes, mean reward 56.996, eps 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413280: done 287 episodes, mean reward 57.060, eps 0.02\n",
      "414720: done 288 episodes, mean reward 57.027, eps 0.02\n",
      "416160: done 289 episodes, mean reward 57.076, eps 0.02\n",
      "417600: done 290 episodes, mean reward 57.125, eps 0.02\n",
      "419040: done 291 episodes, mean reward 57.147, eps 0.02\n",
      "420480: done 292 episodes, mean reward 57.216, eps 0.02\n",
      "421920: done 293 episodes, mean reward 57.255, eps 0.02\n",
      "423360: done 294 episodes, mean reward 57.330, eps 0.02\n",
      "424800: done 295 episodes, mean reward 57.347, eps 0.02\n",
      "426240: done 296 episodes, mean reward 57.496, eps 0.02\n",
      "427680: done 297 episodes, mean reward 57.636, eps 0.02\n",
      "429120: done 298 episodes, mean reward 57.702, eps 0.02\n",
      "430560: done 299 episodes, mean reward 57.760, eps 0.02\n",
      "432000: done 300 episodes, mean reward 57.771, eps 0.02\n",
      "433440: done 301 episodes, mean reward 57.839, eps 0.02\n",
      "434880: done 302 episodes, mean reward 57.951, eps 0.02\n",
      "436320: done 303 episodes, mean reward 57.953, eps 0.02\n",
      "437760: done 304 episodes, mean reward 58.141, eps 0.02\n",
      "439200: done 305 episodes, mean reward 58.126, eps 0.02\n",
      "440640: done 306 episodes, mean reward 58.226, eps 0.02\n",
      "442080: done 307 episodes, mean reward 58.250, eps 0.02\n",
      "443520: done 308 episodes, mean reward 58.245, eps 0.02\n",
      "444960: done 309 episodes, mean reward 58.335, eps 0.02\n",
      "446400: done 310 episodes, mean reward 58.334, eps 0.02\n",
      "447840: done 311 episodes, mean reward 58.510, eps 0.02\n",
      "449280: done 312 episodes, mean reward 58.518, eps 0.02\n",
      "450720: done 313 episodes, mean reward 58.593, eps 0.02\n",
      "452160: done 314 episodes, mean reward 58.670, eps 0.02\n",
      "453600: done 315 episodes, mean reward 58.693, eps 0.02\n",
      "455040: done 316 episodes, mean reward 58.828, eps 0.02\n",
      "456480: done 317 episodes, mean reward 58.863, eps 0.02\n",
      "457920: done 318 episodes, mean reward 59.043, eps 0.02\n",
      "Best mean reward updated 59.023 -> 59.043, model saved\n",
      "459360: done 319 episodes, mean reward 59.227, eps 0.02\n",
      "Best mean reward updated 59.043 -> 59.227, model saved\n",
      "460800: done 320 episodes, mean reward 59.288, eps 0.02\n",
      "Best mean reward updated 59.227 -> 59.288, model saved\n",
      "462240: done 321 episodes, mean reward 59.366, eps 0.02\n",
      "Best mean reward updated 59.288 -> 59.366, model saved\n",
      "463680: done 322 episodes, mean reward 59.474, eps 0.02\n",
      "Best mean reward updated 59.366 -> 59.474, model saved\n",
      "465120: done 323 episodes, mean reward 59.540, eps 0.02\n",
      "Best mean reward updated 59.474 -> 59.540, model saved\n",
      "466560: done 324 episodes, mean reward 59.613, eps 0.02\n",
      "Best mean reward updated 59.540 -> 59.613, model saved\n",
      "468000: done 325 episodes, mean reward 59.680, eps 0.02\n",
      "Best mean reward updated 59.613 -> 59.680, model saved\n",
      "469440: done 326 episodes, mean reward 59.727, eps 0.02\n",
      "Best mean reward updated 59.680 -> 59.727, model saved\n",
      "470880: done 327 episodes, mean reward 59.728, eps 0.02\n",
      "Best mean reward updated 59.727 -> 59.728, model saved\n",
      "472320: done 328 episodes, mean reward 59.796, eps 0.02\n",
      "Best mean reward updated 59.728 -> 59.796, model saved\n",
      "473760: done 329 episodes, mean reward 59.963, eps 0.02\n",
      "Best mean reward updated 59.796 -> 59.963, model saved\n",
      "475200: done 330 episodes, mean reward 60.109, eps 0.02\n",
      "Best mean reward updated 59.963 -> 60.109, model saved\n",
      "476640: done 331 episodes, mean reward 60.112, eps 0.02\n",
      "Best mean reward updated 60.109 -> 60.112, model saved\n",
      "478080: done 332 episodes, mean reward 60.116, eps 0.02\n",
      "Best mean reward updated 60.112 -> 60.116, model saved\n",
      "479520: done 333 episodes, mean reward 60.182, eps 0.02\n",
      "Best mean reward updated 60.116 -> 60.182, model saved\n",
      "480960: done 334 episodes, mean reward 60.190, eps 0.02\n",
      "Best mean reward updated 60.182 -> 60.190, model saved\n",
      "482400: done 335 episodes, mean reward 60.281, eps 0.02\n",
      "Best mean reward updated 60.190 -> 60.281, model saved\n",
      "483840: done 336 episodes, mean reward 60.388, eps 0.02\n",
      "Best mean reward updated 60.281 -> 60.388, model saved\n",
      "485280: done 337 episodes, mean reward 60.476, eps 0.02\n",
      "Best mean reward updated 60.388 -> 60.476, model saved\n",
      "486720: done 338 episodes, mean reward 60.531, eps 0.02\n",
      "Best mean reward updated 60.476 -> 60.531, model saved\n",
      "488160: done 339 episodes, mean reward 60.579, eps 0.02\n",
      "Best mean reward updated 60.531 -> 60.579, model saved\n",
      "489600: done 340 episodes, mean reward 60.635, eps 0.02\n",
      "Best mean reward updated 60.579 -> 60.635, model saved\n",
      "491040: done 341 episodes, mean reward 60.701, eps 0.02\n",
      "Best mean reward updated 60.635 -> 60.701, model saved\n",
      "492480: done 342 episodes, mean reward 60.723, eps 0.02\n",
      "Best mean reward updated 60.701 -> 60.723, model saved\n",
      "493920: done 343 episodes, mean reward 60.744, eps 0.02\n",
      "Best mean reward updated 60.723 -> 60.744, model saved\n",
      "495360: done 344 episodes, mean reward 60.846, eps 0.02\n",
      "Best mean reward updated 60.744 -> 60.846, model saved\n",
      "496800: done 345 episodes, mean reward 60.925, eps 0.02\n",
      "Best mean reward updated 60.846 -> 60.925, model saved\n",
      "498240: done 346 episodes, mean reward 61.006, eps 0.02\n",
      "Best mean reward updated 60.925 -> 61.006, model saved\n",
      "499680: done 347 episodes, mean reward 61.082, eps 0.02\n",
      "Best mean reward updated 61.006 -> 61.082, model saved\n",
      "501120: done 348 episodes, mean reward 61.090, eps 0.02\n",
      "Best mean reward updated 61.082 -> 61.090, model saved\n",
      "502560: done 349 episodes, mean reward 61.295, eps 0.02\n",
      "Best mean reward updated 61.090 -> 61.295, model saved\n",
      "504000: done 350 episodes, mean reward 61.350, eps 0.02\n",
      "Best mean reward updated 61.295 -> 61.350, model saved\n",
      "505440: done 351 episodes, mean reward 61.441, eps 0.02\n",
      "Best mean reward updated 61.350 -> 61.441, model saved\n",
      "506880: done 352 episodes, mean reward 61.464, eps 0.02\n",
      "Best mean reward updated 61.441 -> 61.464, model saved\n",
      "508320: done 353 episodes, mean reward 61.523, eps 0.02\n",
      "Best mean reward updated 61.464 -> 61.523, model saved\n",
      "509760: done 354 episodes, mean reward 61.630, eps 0.02\n",
      "Best mean reward updated 61.523 -> 61.630, model saved\n",
      "511200: done 355 episodes, mean reward 61.683, eps 0.02\n",
      "Best mean reward updated 61.630 -> 61.683, model saved\n",
      "512640: done 356 episodes, mean reward 61.745, eps 0.02\n",
      "Best mean reward updated 61.683 -> 61.745, model saved\n",
      "514080: done 357 episodes, mean reward 61.783, eps 0.02\n",
      "Best mean reward updated 61.745 -> 61.783, model saved\n",
      "515520: done 358 episodes, mean reward 61.880, eps 0.02\n",
      "Best mean reward updated 61.783 -> 61.880, model saved\n",
      "516960: done 359 episodes, mean reward 61.962, eps 0.02\n",
      "Best mean reward updated 61.880 -> 61.962, model saved\n",
      "518400: done 360 episodes, mean reward 62.036, eps 0.02\n",
      "Best mean reward updated 61.962 -> 62.036, model saved\n",
      "519840: done 361 episodes, mean reward 62.068, eps 0.02\n",
      "Best mean reward updated 62.036 -> 62.068, model saved\n",
      "521280: done 362 episodes, mean reward 62.085, eps 0.02\n",
      "Best mean reward updated 62.068 -> 62.085, model saved\n",
      "522720: done 363 episodes, mean reward 62.130, eps 0.02\n",
      "Best mean reward updated 62.085 -> 62.130, model saved\n",
      "524160: done 364 episodes, mean reward 62.234, eps 0.02\n",
      "Best mean reward updated 62.130 -> 62.234, model saved\n",
      "525600: done 365 episodes, mean reward 62.277, eps 0.02\n",
      "Best mean reward updated 62.234 -> 62.277, model saved\n",
      "527040: done 366 episodes, mean reward 62.240, eps 0.02\n",
      "528480: done 367 episodes, mean reward 62.290, eps 0.02\n",
      "Best mean reward updated 62.277 -> 62.290, model saved\n",
      "529920: done 368 episodes, mean reward 62.219, eps 0.02\n",
      "531360: done 369 episodes, mean reward 62.214, eps 0.02\n",
      "532800: done 370 episodes, mean reward 62.058, eps 0.02\n",
      "534240: done 371 episodes, mean reward 62.049, eps 0.02\n",
      "535680: done 372 episodes, mean reward 62.053, eps 0.02\n",
      "537120: done 373 episodes, mean reward 62.126, eps 0.02\n",
      "538560: done 374 episodes, mean reward 62.228, eps 0.02\n",
      "540000: done 375 episodes, mean reward 62.259, eps 0.02\n",
      "541440: done 376 episodes, mean reward 62.344, eps 0.02\n",
      "Best mean reward updated 62.290 -> 62.344, model saved\n",
      "542880: done 377 episodes, mean reward 62.358, eps 0.02\n",
      "Best mean reward updated 62.344 -> 62.358, model saved\n",
      "544320: done 378 episodes, mean reward 62.302, eps 0.02\n",
      "545760: done 379 episodes, mean reward 62.280, eps 0.02\n",
      "547200: done 380 episodes, mean reward 62.298, eps 0.02\n",
      "548640: done 381 episodes, mean reward 62.398, eps 0.02\n",
      "Best mean reward updated 62.358 -> 62.398, model saved\n",
      "550080: done 382 episodes, mean reward 62.409, eps 0.02\n",
      "Best mean reward updated 62.398 -> 62.409, model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551520: done 383 episodes, mean reward 62.518, eps 0.02\n",
      "Best mean reward updated 62.409 -> 62.518, model saved\n",
      "552960: done 384 episodes, mean reward 62.510, eps 0.02\n",
      "554400: done 385 episodes, mean reward 62.514, eps 0.02\n",
      "555840: done 386 episodes, mean reward 62.501, eps 0.02\n",
      "557280: done 387 episodes, mean reward 62.519, eps 0.02\n",
      "Best mean reward updated 62.518 -> 62.519, model saved\n",
      "558720: done 388 episodes, mean reward 62.576, eps 0.02\n",
      "Best mean reward updated 62.519 -> 62.576, model saved\n",
      "560160: done 389 episodes, mean reward 62.593, eps 0.02\n",
      "Best mean reward updated 62.576 -> 62.593, model saved\n",
      "561600: done 390 episodes, mean reward 62.606, eps 0.02\n",
      "Best mean reward updated 62.593 -> 62.606, model saved\n",
      "563040: done 391 episodes, mean reward 62.694, eps 0.02\n",
      "Best mean reward updated 62.606 -> 62.694, model saved\n",
      "564480: done 392 episodes, mean reward 62.707, eps 0.02\n",
      "Best mean reward updated 62.694 -> 62.707, model saved\n",
      "565920: done 393 episodes, mean reward 62.799, eps 0.02\n",
      "Best mean reward updated 62.707 -> 62.799, model saved\n",
      "567360: done 394 episodes, mean reward 62.821, eps 0.02\n",
      "Best mean reward updated 62.799 -> 62.821, model saved\n",
      "568800: done 395 episodes, mean reward 62.953, eps 0.02\n",
      "Best mean reward updated 62.821 -> 62.953, model saved\n",
      "570240: done 396 episodes, mean reward 62.830, eps 0.02\n",
      "571680: done 397 episodes, mean reward 62.845, eps 0.02\n",
      "573120: done 398 episodes, mean reward 62.873, eps 0.02\n",
      "574560: done 399 episodes, mean reward 62.879, eps 0.02\n",
      "576000: done 400 episodes, mean reward 62.944, eps 0.02\n",
      "577440: done 401 episodes, mean reward 62.972, eps 0.02\n",
      "Best mean reward updated 62.953 -> 62.972, model saved\n",
      "578880: done 402 episodes, mean reward 62.897, eps 0.02\n",
      "580320: done 403 episodes, mean reward 63.014, eps 0.02\n",
      "Best mean reward updated 62.972 -> 63.014, model saved\n",
      "581760: done 404 episodes, mean reward 62.978, eps 0.02\n",
      "583200: done 405 episodes, mean reward 63.070, eps 0.02\n",
      "Best mean reward updated 63.014 -> 63.070, model saved\n",
      "584640: done 406 episodes, mean reward 63.059, eps 0.02\n",
      "586080: done 407 episodes, mean reward 63.152, eps 0.02\n",
      "Best mean reward updated 63.070 -> 63.152, model saved\n",
      "587520: done 408 episodes, mean reward 63.129, eps 0.02\n",
      "588960: done 409 episodes, mean reward 63.063, eps 0.02\n",
      "590400: done 410 episodes, mean reward 63.056, eps 0.02\n",
      "591840: done 411 episodes, mean reward 62.888, eps 0.02\n",
      "593280: done 412 episodes, mean reward 62.893, eps 0.02\n",
      "594720: done 413 episodes, mean reward 63.042, eps 0.02\n",
      "596160: done 414 episodes, mean reward 63.072, eps 0.02\n",
      "597600: done 415 episodes, mean reward 63.107, eps 0.02\n",
      "599040: done 416 episodes, mean reward 63.106, eps 0.02\n",
      "600480: done 417 episodes, mean reward 63.273, eps 0.02\n",
      "Best mean reward updated 63.152 -> 63.273, model saved\n",
      "601920: done 418 episodes, mean reward 63.122, eps 0.02\n",
      "603360: done 419 episodes, mean reward 63.094, eps 0.02\n",
      "604800: done 420 episodes, mean reward 63.113, eps 0.02\n",
      "606240: done 421 episodes, mean reward 63.115, eps 0.02\n",
      "607680: done 422 episodes, mean reward 63.128, eps 0.02\n",
      "609120: done 423 episodes, mean reward 63.117, eps 0.02\n",
      "610560: done 424 episodes, mean reward 63.135, eps 0.02\n",
      "612000: done 425 episodes, mean reward 63.201, eps 0.02\n",
      "613440: done 426 episodes, mean reward 63.184, eps 0.02\n",
      "614880: done 427 episodes, mean reward 63.193, eps 0.02\n",
      "616320: done 428 episodes, mean reward 63.223, eps 0.02\n",
      "617760: done 429 episodes, mean reward 63.122, eps 0.02\n",
      "619200: done 430 episodes, mean reward 63.115, eps 0.02\n",
      "620640: done 431 episodes, mean reward 63.203, eps 0.02\n",
      "622080: done 432 episodes, mean reward 63.210, eps 0.02\n",
      "623520: done 433 episodes, mean reward 63.169, eps 0.02\n",
      "624960: done 434 episodes, mean reward 63.205, eps 0.02\n",
      "626400: done 435 episodes, mean reward 63.150, eps 0.02\n",
      "627840: done 436 episodes, mean reward 63.234, eps 0.02\n",
      "629280: done 437 episodes, mean reward 63.209, eps 0.02\n",
      "630720: done 438 episodes, mean reward 63.194, eps 0.02\n",
      "632160: done 439 episodes, mean reward 63.210, eps 0.02\n",
      "633600: done 440 episodes, mean reward 63.245, eps 0.02\n",
      "635040: done 441 episodes, mean reward 63.240, eps 0.02\n",
      "636480: done 442 episodes, mean reward 63.261, eps 0.02\n",
      "637920: done 443 episodes, mean reward 63.324, eps 0.02\n",
      "Best mean reward updated 63.273 -> 63.324, model saved\n",
      "639360: done 444 episodes, mean reward 63.265, eps 0.02\n",
      "640800: done 445 episodes, mean reward 63.239, eps 0.02\n",
      "642240: done 446 episodes, mean reward 63.226, eps 0.02\n",
      "643680: done 447 episodes, mean reward 63.217, eps 0.02\n",
      "645120: done 448 episodes, mean reward 63.260, eps 0.02\n",
      "646560: done 449 episodes, mean reward 63.152, eps 0.02\n",
      "648000: done 450 episodes, mean reward 63.186, eps 0.02\n",
      "649440: done 451 episodes, mean reward 63.239, eps 0.02\n",
      "650880: done 452 episodes, mean reward 63.234, eps 0.02\n",
      "652320: done 453 episodes, mean reward 63.202, eps 0.02\n",
      "653760: done 454 episodes, mean reward 63.219, eps 0.02\n",
      "655200: done 455 episodes, mean reward 63.226, eps 0.02\n",
      "656640: done 456 episodes, mean reward 63.212, eps 0.02\n",
      "658080: done 457 episodes, mean reward 63.231, eps 0.02\n",
      "659520: done 458 episodes, mean reward 63.254, eps 0.02\n",
      "660960: done 459 episodes, mean reward 63.300, eps 0.02\n",
      "662400: done 460 episodes, mean reward 63.313, eps 0.02\n",
      "663840: done 461 episodes, mean reward 63.457, eps 0.02\n",
      "Best mean reward updated 63.324 -> 63.457, model saved\n",
      "665280: done 462 episodes, mean reward 63.438, eps 0.02\n",
      "666720: done 463 episodes, mean reward 63.405, eps 0.02\n",
      "668160: done 464 episodes, mean reward 63.398, eps 0.02\n",
      "669600: done 465 episodes, mean reward 63.417, eps 0.02\n",
      "671040: done 466 episodes, mean reward 63.374, eps 0.02\n",
      "672480: done 467 episodes, mean reward 63.531, eps 0.02\n",
      "Best mean reward updated 63.457 -> 63.531, model saved\n",
      "673920: done 468 episodes, mean reward 63.570, eps 0.02\n",
      "Best mean reward updated 63.531 -> 63.570, model saved\n",
      "675360: done 469 episodes, mean reward 63.566, eps 0.02\n",
      "676800: done 470 episodes, mean reward 63.562, eps 0.02\n",
      "678240: done 471 episodes, mean reward 63.588, eps 0.02\n",
      "Best mean reward updated 63.570 -> 63.588, model saved\n",
      "679680: done 472 episodes, mean reward 63.700, eps 0.02\n",
      "Best mean reward updated 63.588 -> 63.700, model saved\n",
      "681120: done 473 episodes, mean reward 63.678, eps 0.02\n",
      "682560: done 474 episodes, mean reward 63.663, eps 0.02\n",
      "684000: done 475 episodes, mean reward 63.671, eps 0.02\n",
      "685440: done 476 episodes, mean reward 63.615, eps 0.02\n",
      "686880: done 477 episodes, mean reward 63.664, eps 0.02\n",
      "688320: done 478 episodes, mean reward 63.673, eps 0.02\n",
      "689760: done 479 episodes, mean reward 63.658, eps 0.02\n",
      "691200: done 480 episodes, mean reward 63.734, eps 0.02\n",
      "Best mean reward updated 63.700 -> 63.734, model saved\n",
      "692640: done 481 episodes, mean reward 63.650, eps 0.02\n",
      "694080: done 482 episodes, mean reward 63.631, eps 0.02\n",
      "695520: done 483 episodes, mean reward 63.656, eps 0.02\n",
      "696960: done 484 episodes, mean reward 63.651, eps 0.02\n",
      "698400: done 485 episodes, mean reward 63.702, eps 0.02\n",
      "699840: done 486 episodes, mean reward 63.798, eps 0.02\n",
      "Best mean reward updated 63.734 -> 63.798, model saved\n",
      "701280: done 487 episodes, mean reward 63.799, eps 0.02\n",
      "Best mean reward updated 63.798 -> 63.799, model saved\n",
      "702720: done 488 episodes, mean reward 63.860, eps 0.02\n",
      "Best mean reward updated 63.799 -> 63.860, model saved\n",
      "704160: done 489 episodes, mean reward 63.861, eps 0.02\n",
      "Best mean reward updated 63.860 -> 63.861, model saved\n",
      "705600: done 490 episodes, mean reward 63.949, eps 0.02\n",
      "Best mean reward updated 63.861 -> 63.949, model saved\n",
      "707040: done 491 episodes, mean reward 63.961, eps 0.02\n",
      "Best mean reward updated 63.949 -> 63.961, model saved\n",
      "708480: done 492 episodes, mean reward 63.944, eps 0.02\n",
      "709920: done 493 episodes, mean reward 63.890, eps 0.02\n",
      "711360: done 494 episodes, mean reward 63.849, eps 0.02\n",
      "712800: done 495 episodes, mean reward 63.837, eps 0.02\n",
      "714240: done 496 episodes, mean reward 63.931, eps 0.02\n",
      "715680: done 497 episodes, mean reward 63.859, eps 0.02\n",
      "717120: done 498 episodes, mean reward 63.998, eps 0.02\n",
      "Best mean reward updated 63.961 -> 63.998, model saved\n",
      "718560: done 499 episodes, mean reward 63.993, eps 0.02\n",
      "720000: done 500 episodes, mean reward 64.003, eps 0.02\n",
      "Best mean reward updated 63.998 -> 64.003, model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721440: done 501 episodes, mean reward 64.012, eps 0.02\n",
      "Best mean reward updated 64.003 -> 64.012, model saved\n",
      "722880: done 502 episodes, mean reward 64.056, eps 0.02\n",
      "Best mean reward updated 64.012 -> 64.056, model saved\n",
      "724320: done 503 episodes, mean reward 63.996, eps 0.02\n",
      "725760: done 504 episodes, mean reward 63.905, eps 0.02\n",
      "727200: done 505 episodes, mean reward 63.946, eps 0.02\n",
      "728640: done 506 episodes, mean reward 63.905, eps 0.02\n",
      "730080: done 507 episodes, mean reward 63.914, eps 0.02\n",
      "731520: done 508 episodes, mean reward 63.913, eps 0.02\n",
      "732960: done 509 episodes, mean reward 63.911, eps 0.02\n",
      "734400: done 510 episodes, mean reward 63.958, eps 0.02\n",
      "735840: done 511 episodes, mean reward 64.097, eps 0.02\n",
      "Best mean reward updated 64.056 -> 64.097, model saved\n",
      "737280: done 512 episodes, mean reward 64.318, eps 0.02\n",
      "Best mean reward updated 64.097 -> 64.318, model saved\n",
      "738720: done 513 episodes, mean reward 64.302, eps 0.02\n",
      "740160: done 514 episodes, mean reward 64.322, eps 0.02\n",
      "Best mean reward updated 64.318 -> 64.322, model saved\n",
      "741600: done 515 episodes, mean reward 64.516, eps 0.02\n",
      "Best mean reward updated 64.322 -> 64.516, model saved\n",
      "743040: done 516 episodes, mean reward 64.496, eps 0.02\n",
      "744480: done 517 episodes, mean reward 64.385, eps 0.02\n",
      "745920: done 518 episodes, mean reward 64.378, eps 0.02\n",
      "747360: done 519 episodes, mean reward 64.308, eps 0.02\n",
      "748800: done 520 episodes, mean reward 64.328, eps 0.02\n",
      "750240: done 521 episodes, mean reward 64.364, eps 0.02\n",
      "751680: done 522 episodes, mean reward 64.488, eps 0.02\n",
      "753120: done 523 episodes, mean reward 64.510, eps 0.02\n",
      "754560: done 524 episodes, mean reward 64.467, eps 0.02\n",
      "756000: done 525 episodes, mean reward 64.432, eps 0.02\n",
      "757440: done 526 episodes, mean reward 64.404, eps 0.02\n",
      "758880: done 527 episodes, mean reward 64.456, eps 0.02\n",
      "760320: done 528 episodes, mean reward 64.442, eps 0.02\n",
      "761760: done 529 episodes, mean reward 64.490, eps 0.02\n",
      "763200: done 530 episodes, mean reward 64.448, eps 0.02\n",
      "764640: done 531 episodes, mean reward 64.407, eps 0.02\n",
      "766080: done 532 episodes, mean reward 64.470, eps 0.02\n",
      "767520: done 533 episodes, mean reward 64.524, eps 0.02\n",
      "Best mean reward updated 64.516 -> 64.524, model saved\n",
      "768960: done 534 episodes, mean reward 64.499, eps 0.02\n",
      "770400: done 535 episodes, mean reward 64.506, eps 0.02\n",
      "771840: done 536 episodes, mean reward 64.563, eps 0.02\n",
      "Best mean reward updated 64.524 -> 64.563, model saved\n",
      "773280: done 537 episodes, mean reward 64.629, eps 0.02\n",
      "Best mean reward updated 64.563 -> 64.629, model saved\n",
      "774720: done 538 episodes, mean reward 64.641, eps 0.02\n",
      "Best mean reward updated 64.629 -> 64.641, model saved\n",
      "776160: done 539 episodes, mean reward 64.605, eps 0.02\n",
      "777600: done 540 episodes, mean reward 64.585, eps 0.02\n",
      "779040: done 541 episodes, mean reward 64.624, eps 0.02\n",
      "780480: done 542 episodes, mean reward 64.699, eps 0.02\n",
      "Best mean reward updated 64.641 -> 64.699, model saved\n",
      "781920: done 543 episodes, mean reward 64.640, eps 0.02\n",
      "783360: done 544 episodes, mean reward 64.671, eps 0.02\n",
      "784800: done 545 episodes, mean reward 64.646, eps 0.02\n",
      "786240: done 546 episodes, mean reward 64.619, eps 0.02\n",
      "787680: done 547 episodes, mean reward 64.683, eps 0.02\n",
      "789120: done 548 episodes, mean reward 64.622, eps 0.02\n",
      "790560: done 549 episodes, mean reward 64.610, eps 0.02\n",
      "792000: done 550 episodes, mean reward 64.570, eps 0.02\n",
      "793440: done 551 episodes, mean reward 64.550, eps 0.02\n",
      "794880: done 552 episodes, mean reward 64.541, eps 0.02\n",
      "796320: done 553 episodes, mean reward 64.539, eps 0.02\n",
      "797760: done 554 episodes, mean reward 64.519, eps 0.02\n",
      "799200: done 555 episodes, mean reward 64.603, eps 0.02\n",
      "800640: done 556 episodes, mean reward 64.683, eps 0.02\n",
      "802080: done 557 episodes, mean reward 64.758, eps 0.02\n",
      "Best mean reward updated 64.699 -> 64.758, model saved\n",
      "803520: done 558 episodes, mean reward 64.789, eps 0.02\n",
      "Best mean reward updated 64.758 -> 64.789, model saved\n",
      "804960: done 559 episodes, mean reward 64.808, eps 0.02\n",
      "Best mean reward updated 64.789 -> 64.808, model saved\n",
      "806400: done 560 episodes, mean reward 64.796, eps 0.02\n",
      "807840: done 561 episodes, mean reward 64.722, eps 0.02\n",
      "809280: done 562 episodes, mean reward 64.700, eps 0.02\n",
      "810720: done 563 episodes, mean reward 64.742, eps 0.02\n",
      "812160: done 564 episodes, mean reward 64.752, eps 0.02\n",
      "813600: done 565 episodes, mean reward 64.790, eps 0.02\n",
      "815040: done 566 episodes, mean reward 64.802, eps 0.02\n",
      "816480: done 567 episodes, mean reward 64.692, eps 0.02\n",
      "817920: done 568 episodes, mean reward 64.699, eps 0.02\n",
      "819360: done 569 episodes, mean reward 64.693, eps 0.02\n",
      "820800: done 570 episodes, mean reward 64.660, eps 0.02\n",
      "822240: done 571 episodes, mean reward 64.695, eps 0.02\n",
      "823680: done 572 episodes, mean reward 64.640, eps 0.02\n",
      "825120: done 573 episodes, mean reward 64.664, eps 0.02\n",
      "826560: done 574 episodes, mean reward 64.702, eps 0.02\n",
      "828000: done 575 episodes, mean reward 64.832, eps 0.02\n",
      "Best mean reward updated 64.808 -> 64.832, model saved\n",
      "829440: done 576 episodes, mean reward 64.854, eps 0.02\n",
      "Best mean reward updated 64.832 -> 64.854, model saved\n",
      "830880: done 577 episodes, mean reward 64.734, eps 0.02\n",
      "832320: done 578 episodes, mean reward 64.754, eps 0.02\n",
      "833760: done 579 episodes, mean reward 64.745, eps 0.02\n",
      "835200: done 580 episodes, mean reward 64.656, eps 0.02\n",
      "836640: done 581 episodes, mean reward 64.769, eps 0.02\n",
      "838080: done 582 episodes, mean reward 64.789, eps 0.02\n",
      "839520: done 583 episodes, mean reward 64.794, eps 0.02\n",
      "840960: done 584 episodes, mean reward 64.774, eps 0.02\n",
      "842400: done 585 episodes, mean reward 64.813, eps 0.02\n",
      "843840: done 586 episodes, mean reward 64.814, eps 0.02\n",
      "845280: done 587 episodes, mean reward 64.869, eps 0.02\n",
      "Best mean reward updated 64.854 -> 64.869, model saved\n",
      "846720: done 588 episodes, mean reward 64.810, eps 0.02\n",
      "848160: done 589 episodes, mean reward 64.789, eps 0.02\n",
      "849600: done 590 episodes, mean reward 64.738, eps 0.02\n",
      "851040: done 591 episodes, mean reward 64.677, eps 0.02\n",
      "852480: done 592 episodes, mean reward 64.721, eps 0.02\n",
      "853920: done 593 episodes, mean reward 64.742, eps 0.02\n",
      "855360: done 594 episodes, mean reward 64.804, eps 0.02\n",
      "856800: done 595 episodes, mean reward 64.755, eps 0.02\n",
      "858240: done 596 episodes, mean reward 64.717, eps 0.02\n",
      "859680: done 597 episodes, mean reward 64.705, eps 0.02\n",
      "861120: done 598 episodes, mean reward 64.547, eps 0.02\n",
      "862560: done 599 episodes, mean reward 64.571, eps 0.02\n",
      "864000: done 600 episodes, mean reward 64.649, eps 0.02\n",
      "865440: done 601 episodes, mean reward 64.589, eps 0.02\n",
      "866880: done 602 episodes, mean reward 64.571, eps 0.02\n",
      "868320: done 603 episodes, mean reward 64.569, eps 0.02\n",
      "869760: done 604 episodes, mean reward 64.578, eps 0.02\n",
      "871200: done 605 episodes, mean reward 64.534, eps 0.02\n",
      "872640: done 606 episodes, mean reward 64.563, eps 0.02\n",
      "874080: done 607 episodes, mean reward 64.630, eps 0.02\n",
      "875520: done 608 episodes, mean reward 64.622, eps 0.02\n",
      "876960: done 609 episodes, mean reward 64.666, eps 0.02\n",
      "878400: done 610 episodes, mean reward 64.668, eps 0.02\n",
      "879840: done 611 episodes, mean reward 64.555, eps 0.02\n",
      "881280: done 612 episodes, mean reward 64.381, eps 0.02\n",
      "882720: done 613 episodes, mean reward 64.284, eps 0.02\n",
      "884160: done 614 episodes, mean reward 64.292, eps 0.02\n",
      "885600: done 615 episodes, mean reward 64.119, eps 0.02\n",
      "887040: done 616 episodes, mean reward 64.076, eps 0.02\n",
      "888480: done 617 episodes, mean reward 64.059, eps 0.02\n",
      "889920: done 618 episodes, mean reward 64.089, eps 0.02\n",
      "891360: done 619 episodes, mean reward 64.175, eps 0.02\n",
      "892800: done 620 episodes, mean reward 64.137, eps 0.02\n",
      "894240: done 621 episodes, mean reward 64.119, eps 0.02\n",
      "895680: done 622 episodes, mean reward 63.960, eps 0.02\n",
      "897120: done 623 episodes, mean reward 63.957, eps 0.02\n",
      "898560: done 624 episodes, mean reward 64.003, eps 0.02\n",
      "900000: done 625 episodes, mean reward 63.988, eps 0.02\n",
      "901440: done 626 episodes, mean reward 64.045, eps 0.02\n",
      "902880: done 627 episodes, mean reward 64.050, eps 0.02\n",
      "904320: done 628 episodes, mean reward 64.097, eps 0.02\n",
      "905760: done 629 episodes, mean reward 64.039, eps 0.02\n",
      "907200: done 630 episodes, mean reward 64.026, eps 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908640: done 631 episodes, mean reward 64.009, eps 0.02\n",
      "910080: done 632 episodes, mean reward 63.996, eps 0.02\n",
      "911520: done 633 episodes, mean reward 64.043, eps 0.02\n",
      "912960: done 634 episodes, mean reward 64.047, eps 0.02\n",
      "914400: done 635 episodes, mean reward 64.065, eps 0.02\n",
      "915840: done 636 episodes, mean reward 63.894, eps 0.02\n",
      "917280: done 637 episodes, mean reward 63.834, eps 0.02\n",
      "918720: done 638 episodes, mean reward 63.857, eps 0.02\n",
      "920160: done 639 episodes, mean reward 63.908, eps 0.02\n",
      "921600: done 640 episodes, mean reward 63.888, eps 0.02\n",
      "923040: done 641 episodes, mean reward 63.862, eps 0.02\n",
      "924480: done 642 episodes, mean reward 63.780, eps 0.02\n",
      "925920: done 643 episodes, mean reward 63.810, eps 0.02\n",
      "927360: done 644 episodes, mean reward 63.947, eps 0.02\n",
      "928800: done 645 episodes, mean reward 63.922, eps 0.02\n",
      "930240: done 646 episodes, mean reward 63.967, eps 0.02\n",
      "931680: done 647 episodes, mean reward 63.897, eps 0.02\n",
      "933120: done 648 episodes, mean reward 64.046, eps 0.02\n",
      "934560: done 649 episodes, mean reward 64.113, eps 0.02\n",
      "936000: done 650 episodes, mean reward 64.095, eps 0.02\n",
      "937440: done 651 episodes, mean reward 64.044, eps 0.02\n",
      "938880: done 652 episodes, mean reward 64.098, eps 0.02\n",
      "940320: done 653 episodes, mean reward 64.120, eps 0.02\n",
      "941760: done 654 episodes, mean reward 64.215, eps 0.02\n",
      "943200: done 655 episodes, mean reward 64.151, eps 0.02\n",
      "944640: done 656 episodes, mean reward 64.199, eps 0.02\n",
      "946080: done 657 episodes, mean reward 64.127, eps 0.02\n",
      "947520: done 658 episodes, mean reward 64.170, eps 0.02\n",
      "948960: done 659 episodes, mean reward 64.100, eps 0.02\n",
      "950400: done 660 episodes, mean reward 64.116, eps 0.02\n",
      "951840: done 661 episodes, mean reward 64.123, eps 0.02\n",
      "953280: done 662 episodes, mean reward 64.142, eps 0.02\n",
      "954720: done 663 episodes, mean reward 64.145, eps 0.02\n",
      "956160: done 664 episodes, mean reward 64.138, eps 0.02\n",
      "957600: done 665 episodes, mean reward 64.076, eps 0.02\n",
      "959040: done 666 episodes, mean reward 64.133, eps 0.02\n",
      "960480: done 667 episodes, mean reward 64.073, eps 0.02\n",
      "961920: done 668 episodes, mean reward 64.241, eps 0.02\n",
      "963360: done 669 episodes, mean reward 64.271, eps 0.02\n",
      "964800: done 670 episodes, mean reward 64.289, eps 0.02\n",
      "966240: done 671 episodes, mean reward 64.314, eps 0.02\n",
      "967680: done 672 episodes, mean reward 64.342, eps 0.02\n",
      "969120: done 673 episodes, mean reward 64.406, eps 0.02\n",
      "970560: done 674 episodes, mean reward 64.358, eps 0.02\n",
      "972000: done 675 episodes, mean reward 64.228, eps 0.02\n",
      "973440: done 676 episodes, mean reward 64.229, eps 0.02\n",
      "974880: done 677 episodes, mean reward 64.359, eps 0.02\n",
      "976320: done 678 episodes, mean reward 64.380, eps 0.02\n",
      "977760: done 679 episodes, mean reward 64.408, eps 0.02\n",
      "979200: done 680 episodes, mean reward 64.414, eps 0.02\n",
      "980640: done 681 episodes, mean reward 64.453, eps 0.02\n",
      "982080: done 682 episodes, mean reward 64.469, eps 0.02\n",
      "983520: done 683 episodes, mean reward 64.404, eps 0.02\n",
      "984960: done 684 episodes, mean reward 64.389, eps 0.02\n",
      "986400: done 685 episodes, mean reward 64.334, eps 0.02\n",
      "987840: done 686 episodes, mean reward 64.289, eps 0.02\n",
      "989280: done 687 episodes, mean reward 64.383, eps 0.02\n",
      "990720: done 688 episodes, mean reward 64.384, eps 0.02\n",
      "992160: done 689 episodes, mean reward 64.402, eps 0.02\n",
      "993600: done 690 episodes, mean reward 64.414, eps 0.02\n",
      "995040: done 691 episodes, mean reward 64.453, eps 0.02\n",
      "996480: done 692 episodes, mean reward 64.445, eps 0.02\n",
      "997920: done 693 episodes, mean reward 64.398, eps 0.02\n",
      "999360: done 694 episodes, mean reward 64.346, eps 0.02\n",
      "1000800: done 695 episodes, mean reward 64.395, eps 0.02\n",
      "1002240: done 696 episodes, mean reward 64.373, eps 0.02\n",
      "1003680: done 697 episodes, mean reward 64.410, eps 0.02\n",
      "1005120: done 698 episodes, mean reward 64.465, eps 0.02\n",
      "1006560: done 699 episodes, mean reward 64.472, eps 0.02\n",
      "1008000: done 700 episodes, mean reward 64.391, eps 0.02\n",
      "1009440: done 701 episodes, mean reward 64.409, eps 0.02\n",
      "1010880: done 702 episodes, mean reward 64.390, eps 0.02\n",
      "1012320: done 703 episodes, mean reward 64.456, eps 0.02\n",
      "1013760: done 704 episodes, mean reward 64.557, eps 0.02\n",
      "1015200: done 705 episodes, mean reward 64.608, eps 0.02\n",
      "1016640: done 706 episodes, mean reward 64.550, eps 0.02\n",
      "1018080: done 707 episodes, mean reward 64.477, eps 0.02\n",
      "1019520: done 708 episodes, mean reward 64.474, eps 0.02\n",
      "1020960: done 709 episodes, mean reward 64.568, eps 0.02\n",
      "1022400: done 710 episodes, mean reward 64.540, eps 0.02\n",
      "1023840: done 711 episodes, mean reward 64.592, eps 0.02\n",
      "1025280: done 712 episodes, mean reward 64.627, eps 0.02\n",
      "1026720: done 713 episodes, mean reward 64.611, eps 0.02\n",
      "1028160: done 714 episodes, mean reward 64.554, eps 0.02\n",
      "1029600: done 715 episodes, mean reward 64.509, eps 0.02\n",
      "1031040: done 716 episodes, mean reward 64.548, eps 0.02\n",
      "1032480: done 717 episodes, mean reward 64.482, eps 0.02\n",
      "1033920: done 718 episodes, mean reward 64.430, eps 0.02\n",
      "1035360: done 719 episodes, mean reward 64.364, eps 0.02\n",
      "1036800: done 720 episodes, mean reward 64.389, eps 0.02\n",
      "1038240: done 721 episodes, mean reward 64.372, eps 0.02\n",
      "1039680: done 722 episodes, mean reward 64.404, eps 0.02\n",
      "1041120: done 723 episodes, mean reward 64.424, eps 0.02\n",
      "1042560: done 724 episodes, mean reward 64.398, eps 0.02\n",
      "1044000: done 725 episodes, mean reward 64.398, eps 0.02\n",
      "1045440: done 726 episodes, mean reward 64.401, eps 0.02\n",
      "1046880: done 727 episodes, mean reward 64.415, eps 0.02\n",
      "1048320: done 728 episodes, mean reward 64.374, eps 0.02\n",
      "1049760: done 729 episodes, mean reward 64.412, eps 0.02\n",
      "1051200: done 730 episodes, mean reward 64.446, eps 0.02\n",
      "1052640: done 731 episodes, mean reward 64.461, eps 0.02\n",
      "1054080: done 732 episodes, mean reward 64.422, eps 0.02\n",
      "1055520: done 733 episodes, mean reward 64.368, eps 0.02\n",
      "1056960: done 734 episodes, mean reward 64.372, eps 0.02\n",
      "1058400: done 735 episodes, mean reward 64.501, eps 0.02\n",
      "1059840: done 736 episodes, mean reward 64.471, eps 0.02\n",
      "1061280: done 737 episodes, mean reward 64.486, eps 0.02\n",
      "1062720: done 738 episodes, mean reward 64.440, eps 0.02\n",
      "1064160: done 739 episodes, mean reward 64.357, eps 0.02\n",
      "1065600: done 740 episodes, mean reward 64.406, eps 0.02\n",
      "1067040: done 741 episodes, mean reward 64.555, eps 0.02\n",
      "1068480: done 742 episodes, mean reward 64.564, eps 0.02\n",
      "1069920: done 743 episodes, mean reward 64.523, eps 0.02\n",
      "1071360: done 744 episodes, mean reward 64.353, eps 0.02\n",
      "1072800: done 745 episodes, mean reward 64.428, eps 0.02\n",
      "1074240: done 746 episodes, mean reward 64.437, eps 0.02\n",
      "1075680: done 747 episodes, mean reward 64.514, eps 0.02\n",
      "1077120: done 748 episodes, mean reward 64.403, eps 0.02\n",
      "1078560: done 749 episodes, mean reward 64.365, eps 0.02\n",
      "1080000: done 750 episodes, mean reward 64.358, eps 0.02\n",
      "1081440: done 751 episodes, mean reward 64.313, eps 0.02\n",
      "1082880: done 752 episodes, mean reward 64.352, eps 0.02\n",
      "1084320: done 753 episodes, mean reward 64.399, eps 0.02\n",
      "1085760: done 754 episodes, mean reward 64.325, eps 0.02\n",
      "1087200: done 755 episodes, mean reward 64.347, eps 0.02\n",
      "1088640: done 756 episodes, mean reward 64.226, eps 0.02\n",
      "1090080: done 757 episodes, mean reward 64.259, eps 0.02\n",
      "1091520: done 758 episodes, mean reward 64.186, eps 0.02\n",
      "1092960: done 759 episodes, mean reward 64.187, eps 0.02\n",
      "1094400: done 760 episodes, mean reward 64.212, eps 0.02\n",
      "1095840: done 761 episodes, mean reward 64.171, eps 0.02\n",
      "1097280: done 762 episodes, mean reward 64.201, eps 0.02\n",
      "1098720: done 763 episodes, mean reward 64.167, eps 0.02\n",
      "1100160: done 764 episodes, mean reward 64.127, eps 0.02\n",
      "1101600: done 765 episodes, mean reward 64.135, eps 0.02\n",
      "1103040: done 766 episodes, mean reward 64.126, eps 0.02\n",
      "1104480: done 767 episodes, mean reward 64.152, eps 0.02\n",
      "1105920: done 768 episodes, mean reward 64.017, eps 0.02\n",
      "1107360: done 769 episodes, mean reward 64.081, eps 0.02\n",
      "1108800: done 770 episodes, mean reward 64.091, eps 0.02\n",
      "1110240: done 771 episodes, mean reward 64.093, eps 0.02\n",
      "1111680: done 772 episodes, mean reward 64.052, eps 0.02\n",
      "1113120: done 773 episodes, mean reward 64.006, eps 0.02\n",
      "1114560: done 774 episodes, mean reward 64.019, eps 0.02\n",
      "1116000: done 775 episodes, mean reward 64.059, eps 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1117440: done 776 episodes, mean reward 64.080, eps 0.02\n",
      "1118880: done 777 episodes, mean reward 64.167, eps 0.02\n",
      "1120320: done 778 episodes, mean reward 64.153, eps 0.02\n",
      "1121760: done 779 episodes, mean reward 64.182, eps 0.02\n",
      "1123200: done 780 episodes, mean reward 64.164, eps 0.02\n",
      "1124640: done 781 episodes, mean reward 64.066, eps 0.02\n",
      "1126080: done 782 episodes, mean reward 64.077, eps 0.02\n",
      "1127520: done 783 episodes, mean reward 64.096, eps 0.02\n",
      "1128960: done 784 episodes, mean reward 64.112, eps 0.02\n",
      "1130400: done 785 episodes, mean reward 64.002, eps 0.02\n",
      "1131840: done 786 episodes, mean reward 63.923, eps 0.02\n",
      "1133280: done 787 episodes, mean reward 63.800, eps 0.02\n",
      "1134720: done 788 episodes, mean reward 63.859, eps 0.02\n",
      "1136160: done 789 episodes, mean reward 63.847, eps 0.02\n",
      "1137600: done 790 episodes, mean reward 64.042, eps 0.02\n",
      "1139040: done 791 episodes, mean reward 64.038, eps 0.02\n",
      "1140480: done 792 episodes, mean reward 64.052, eps 0.02\n",
      "1141920: done 793 episodes, mean reward 64.045, eps 0.02\n",
      "1143360: done 794 episodes, mean reward 64.056, eps 0.02\n",
      "1144800: done 795 episodes, mean reward 64.037, eps 0.02\n",
      "1146240: done 796 episodes, mean reward 64.038, eps 0.02\n",
      "1147680: done 797 episodes, mean reward 64.051, eps 0.02\n",
      "1149120: done 798 episodes, mean reward 64.015, eps 0.02\n",
      "1150560: done 799 episodes, mean reward 64.315, eps 0.02\n",
      "1152000: done 800 episodes, mean reward 64.292, eps 0.02\n",
      "1153440: done 801 episodes, mean reward 64.322, eps 0.02\n",
      "1154880: done 802 episodes, mean reward 64.317, eps 0.02\n",
      "1156320: done 803 episodes, mean reward 64.282, eps 0.02\n",
      "1157760: done 804 episodes, mean reward 64.157, eps 0.02\n",
      "1159200: done 805 episodes, mean reward 64.212, eps 0.02\n",
      "1160640: done 806 episodes, mean reward 64.237, eps 0.02\n",
      "1162080: done 807 episodes, mean reward 64.233, eps 0.02\n",
      "1163520: done 808 episodes, mean reward 64.267, eps 0.02\n",
      "1164960: done 809 episodes, mean reward 64.200, eps 0.02\n",
      "1166400: done 810 episodes, mean reward 64.242, eps 0.02\n",
      "1167840: done 811 episodes, mean reward 64.236, eps 0.02\n",
      "1169280: done 812 episodes, mean reward 64.219, eps 0.02\n",
      "1170720: done 813 episodes, mean reward 64.228, eps 0.02\n",
      "1172160: done 814 episodes, mean reward 64.195, eps 0.02\n",
      "1173600: done 815 episodes, mean reward 64.284, eps 0.02\n",
      "1175040: done 816 episodes, mean reward 64.249, eps 0.02\n",
      "1176480: done 817 episodes, mean reward 64.342, eps 0.02\n",
      "1177920: done 818 episodes, mean reward 64.418, eps 0.02\n",
      "1179360: done 819 episodes, mean reward 64.355, eps 0.02\n",
      "1180800: done 820 episodes, mean reward 64.353, eps 0.02\n",
      "1182240: done 821 episodes, mean reward 64.350, eps 0.02\n",
      "1183680: done 822 episodes, mean reward 64.277, eps 0.02\n",
      "1185120: done 823 episodes, mean reward 64.257, eps 0.02\n",
      "1186560: done 824 episodes, mean reward 64.232, eps 0.02\n",
      "1188000: done 825 episodes, mean reward 64.213, eps 0.02\n",
      "1189440: done 826 episodes, mean reward 64.210, eps 0.02\n",
      "1190880: done 827 episodes, mean reward 64.247, eps 0.02\n",
      "1192320: done 828 episodes, mean reward 64.231, eps 0.02\n",
      "1193760: done 829 episodes, mean reward 64.301, eps 0.02\n",
      "1195200: done 830 episodes, mean reward 64.278, eps 0.02\n",
      "1196640: done 831 episodes, mean reward 64.292, eps 0.02\n",
      "1198080: done 832 episodes, mean reward 64.326, eps 0.02\n",
      "1199520: done 833 episodes, mean reward 64.364, eps 0.02\n",
      "1200960: done 834 episodes, mean reward 64.275, eps 0.02\n",
      "1202400: done 835 episodes, mean reward 64.157, eps 0.02\n",
      "1203840: done 836 episodes, mean reward 64.200, eps 0.02\n",
      "1205280: done 837 episodes, mean reward 64.216, eps 0.02\n",
      "1206720: done 838 episodes, mean reward 64.244, eps 0.02\n",
      "1208160: done 839 episodes, mean reward 64.280, eps 0.02\n",
      "1209600: done 840 episodes, mean reward 64.371, eps 0.02\n",
      "1211040: done 841 episodes, mean reward 64.233, eps 0.02\n",
      "1212480: done 842 episodes, mean reward 64.253, eps 0.02\n",
      "1213920: done 843 episodes, mean reward 64.332, eps 0.02\n",
      "1215360: done 844 episodes, mean reward 64.350, eps 0.02\n",
      "1216800: done 845 episodes, mean reward 64.296, eps 0.02\n",
      "1218240: done 846 episodes, mean reward 64.279, eps 0.02\n",
      "1219680: done 847 episodes, mean reward 64.241, eps 0.02\n",
      "1221120: done 848 episodes, mean reward 64.224, eps 0.02\n",
      "1222560: done 849 episodes, mean reward 64.127, eps 0.02\n",
      "1224000: done 850 episodes, mean reward 64.173, eps 0.02\n",
      "1225440: done 851 episodes, mean reward 64.232, eps 0.02\n",
      "1226880: done 852 episodes, mean reward 64.163, eps 0.02\n",
      "1228320: done 853 episodes, mean reward 64.115, eps 0.02\n",
      "1229760: done 854 episodes, mean reward 64.090, eps 0.02\n",
      "1231200: done 855 episodes, mean reward 64.054, eps 0.02\n",
      "1232640: done 856 episodes, mean reward 64.032, eps 0.02\n",
      "1234080: done 857 episodes, mean reward 63.986, eps 0.02\n",
      "1235520: done 858 episodes, mean reward 63.953, eps 0.02\n",
      "1236960: done 859 episodes, mean reward 63.972, eps 0.02\n",
      "1238400: done 860 episodes, mean reward 63.915, eps 0.02\n",
      "1239840: done 861 episodes, mean reward 63.999, eps 0.02\n",
      "1241280: done 862 episodes, mean reward 63.966, eps 0.02\n",
      "1242720: done 863 episodes, mean reward 63.983, eps 0.02\n",
      "1244160: done 864 episodes, mean reward 64.016, eps 0.02\n",
      "1245600: done 865 episodes, mean reward 63.962, eps 0.02\n",
      "1247040: done 866 episodes, mean reward 63.909, eps 0.02\n",
      "1248480: done 867 episodes, mean reward 63.933, eps 0.02\n",
      "1249920: done 868 episodes, mean reward 63.872, eps 0.02\n",
      "1251360: done 869 episodes, mean reward 63.834, eps 0.02\n",
      "1252800: done 870 episodes, mean reward 63.820, eps 0.02\n",
      "1254240: done 871 episodes, mean reward 63.809, eps 0.02\n",
      "1255680: done 872 episodes, mean reward 63.771, eps 0.02\n",
      "1257120: done 873 episodes, mean reward 63.858, eps 0.02\n",
      "1258560: done 874 episodes, mean reward 63.881, eps 0.02\n",
      "1260000: done 875 episodes, mean reward 63.929, eps 0.02\n",
      "1261440: done 876 episodes, mean reward 63.868, eps 0.02\n",
      "1262880: done 877 episodes, mean reward 63.740, eps 0.02\n",
      "1264320: done 878 episodes, mean reward 63.733, eps 0.02\n",
      "1265760: done 879 episodes, mean reward 63.638, eps 0.02\n",
      "1267200: done 880 episodes, mean reward 63.706, eps 0.02\n",
      "1268640: done 881 episodes, mean reward 63.689, eps 0.02\n",
      "1270080: done 882 episodes, mean reward 63.690, eps 0.02\n",
      "1271520: done 883 episodes, mean reward 63.631, eps 0.02\n",
      "1272960: done 884 episodes, mean reward 63.691, eps 0.02\n",
      "1274400: done 885 episodes, mean reward 63.752, eps 0.02\n",
      "1275840: done 886 episodes, mean reward 63.816, eps 0.02\n",
      "1277280: done 887 episodes, mean reward 63.812, eps 0.02\n",
      "1278720: done 888 episodes, mean reward 63.735, eps 0.02\n",
      "1280160: done 889 episodes, mean reward 63.790, eps 0.02\n",
      "1281600: done 890 episodes, mean reward 63.649, eps 0.02\n",
      "1283040: done 891 episodes, mean reward 63.609, eps 0.02\n",
      "1284480: done 892 episodes, mean reward 63.629, eps 0.02\n",
      "1285920: done 893 episodes, mean reward 63.656, eps 0.02\n",
      "1287360: done 894 episodes, mean reward 63.780, eps 0.02\n",
      "1288800: done 895 episodes, mean reward 63.769, eps 0.02\n",
      "1290240: done 896 episodes, mean reward 63.798, eps 0.02\n",
      "1291680: done 897 episodes, mean reward 63.768, eps 0.02\n",
      "1293120: done 898 episodes, mean reward 63.914, eps 0.02\n",
      "1294560: done 899 episodes, mean reward 63.653, eps 0.02\n",
      "1296000: done 900 episodes, mean reward 63.798, eps 0.02\n",
      "1297440: done 901 episodes, mean reward 63.797, eps 0.02\n",
      "1298880: done 902 episodes, mean reward 63.813, eps 0.02\n",
      "1300320: done 903 episodes, mean reward 63.844, eps 0.02\n",
      "1301760: done 904 episodes, mean reward 63.808, eps 0.02\n",
      "1303200: done 905 episodes, mean reward 63.799, eps 0.02\n",
      "1304640: done 906 episodes, mean reward 63.831, eps 0.02\n",
      "1306080: done 907 episodes, mean reward 63.829, eps 0.02\n",
      "1307520: done 908 episodes, mean reward 63.906, eps 0.02\n",
      "1308960: done 909 episodes, mean reward 63.887, eps 0.02\n",
      "1310400: done 910 episodes, mean reward 63.831, eps 0.02\n",
      "1311840: done 911 episodes, mean reward 63.849, eps 0.02\n",
      "1313280: done 912 episodes, mean reward 63.794, eps 0.02\n",
      "1314720: done 913 episodes, mean reward 63.770, eps 0.02\n",
      "1316160: done 914 episodes, mean reward 63.778, eps 0.02\n",
      "1317600: done 915 episodes, mean reward 63.718, eps 0.02\n",
      "1319040: done 916 episodes, mean reward 63.809, eps 0.02\n",
      "1320480: done 917 episodes, mean reward 63.822, eps 0.02\n",
      "1321920: done 918 episodes, mean reward 63.812, eps 0.02\n",
      "1323360: done 919 episodes, mean reward 63.857, eps 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1324800: done 920 episodes, mean reward 64.046, eps 0.02\n",
      "1326240: done 921 episodes, mean reward 64.072, eps 0.02\n",
      "1327680: done 922 episodes, mean reward 64.084, eps 0.02\n",
      "1329120: done 923 episodes, mean reward 64.048, eps 0.02\n",
      "1330560: done 924 episodes, mean reward 64.080, eps 0.02\n",
      "1332000: done 925 episodes, mean reward 64.061, eps 0.02\n",
      "1333440: done 926 episodes, mean reward 64.086, eps 0.02\n",
      "1334880: done 927 episodes, mean reward 64.126, eps 0.02\n",
      "1336320: done 928 episodes, mean reward 64.140, eps 0.02\n",
      "1337760: done 929 episodes, mean reward 64.023, eps 0.02\n",
      "1339200: done 930 episodes, mean reward 64.082, eps 0.02\n",
      "1340640: done 931 episodes, mean reward 64.105, eps 0.02\n",
      "1342080: done 932 episodes, mean reward 64.102, eps 0.02\n",
      "1343520: done 933 episodes, mean reward 64.096, eps 0.02\n",
      "1344960: done 934 episodes, mean reward 64.185, eps 0.02\n",
      "1346400: done 935 episodes, mean reward 64.209, eps 0.02\n",
      "1347840: done 936 episodes, mean reward 64.226, eps 0.02\n",
      "1349280: done 937 episodes, mean reward 64.229, eps 0.02\n",
      "1350720: done 938 episodes, mean reward 64.247, eps 0.02\n",
      "1352160: done 939 episodes, mean reward 64.229, eps 0.02\n",
      "1353600: done 940 episodes, mean reward 64.101, eps 0.02\n",
      "1355040: done 941 episodes, mean reward 64.059, eps 0.02\n",
      "1356480: done 942 episodes, mean reward 64.063, eps 0.02\n",
      "1357920: done 943 episodes, mean reward 64.031, eps 0.02\n",
      "1359360: done 944 episodes, mean reward 64.062, eps 0.02\n",
      "1360800: done 945 episodes, mean reward 64.121, eps 0.02\n",
      "1362240: done 946 episodes, mean reward 64.093, eps 0.02\n",
      "1363680: done 947 episodes, mean reward 64.008, eps 0.02\n",
      "1365120: done 948 episodes, mean reward 64.066, eps 0.02\n",
      "1366560: done 949 episodes, mean reward 64.142, eps 0.02\n",
      "1368000: done 950 episodes, mean reward 64.186, eps 0.02\n",
      "1369440: done 951 episodes, mean reward 64.168, eps 0.02\n",
      "1370880: done 952 episodes, mean reward 64.203, eps 0.02\n",
      "1372320: done 953 episodes, mean reward 64.208, eps 0.02\n",
      "1373760: done 954 episodes, mean reward 64.228, eps 0.02\n",
      "1375200: done 955 episodes, mean reward 64.229, eps 0.02\n",
      "1376640: done 956 episodes, mean reward 64.311, eps 0.02\n",
      "1378080: done 957 episodes, mean reward 64.351, eps 0.02\n",
      "1379520: done 958 episodes, mean reward 64.395, eps 0.02\n",
      "1380960: done 959 episodes, mean reward 64.411, eps 0.02\n",
      "1382400: done 960 episodes, mean reward 64.541, eps 0.02\n",
      "1383840: done 961 episodes, mean reward 64.584, eps 0.02\n",
      "1385280: done 962 episodes, mean reward 64.527, eps 0.02\n",
      "1386720: done 963 episodes, mean reward 64.563, eps 0.02\n",
      "1388160: done 964 episodes, mean reward 64.619, eps 0.02\n",
      "1389600: done 965 episodes, mean reward 64.673, eps 0.02\n",
      "1391040: done 966 episodes, mean reward 64.640, eps 0.02\n",
      "1392480: done 967 episodes, mean reward 64.659, eps 0.02\n",
      "1393920: done 968 episodes, mean reward 64.700, eps 0.02\n",
      "1395360: done 969 episodes, mean reward 64.765, eps 0.02\n",
      "1396800: done 970 episodes, mean reward 64.759, eps 0.02\n",
      "1398240: done 971 episodes, mean reward 64.876, eps 0.02\n",
      "Best mean reward updated 64.869 -> 64.876, model saved\n",
      "1399680: done 972 episodes, mean reward 64.857, eps 0.02\n",
      "1401120: done 973 episodes, mean reward 64.800, eps 0.02\n",
      "1402560: done 974 episodes, mean reward 64.914, eps 0.02\n",
      "Best mean reward updated 64.876 -> 64.914, model saved\n",
      "1404000: done 975 episodes, mean reward 64.853, eps 0.02\n",
      "1405440: done 976 episodes, mean reward 64.882, eps 0.02\n",
      "1406880: done 977 episodes, mean reward 64.898, eps 0.02\n",
      "1408320: done 978 episodes, mean reward 65.018, eps 0.02\n",
      "Best mean reward updated 64.914 -> 65.018, model saved\n",
      "1409760: done 979 episodes, mean reward 65.068, eps 0.02\n",
      "Best mean reward updated 65.018 -> 65.068, model saved\n",
      "1411200: done 980 episodes, mean reward 65.045, eps 0.02\n",
      "1412640: done 981 episodes, mean reward 65.025, eps 0.02\n",
      "1414080: done 982 episodes, mean reward 65.020, eps 0.02\n",
      "1415520: done 983 episodes, mean reward 64.977, eps 0.02\n",
      "1416960: done 984 episodes, mean reward 64.938, eps 0.02\n",
      "1418400: done 985 episodes, mean reward 65.045, eps 0.02\n",
      "1419840: done 986 episodes, mean reward 65.028, eps 0.02\n",
      "1421280: done 987 episodes, mean reward 65.090, eps 0.02\n",
      "Best mean reward updated 65.068 -> 65.090, model saved\n",
      "1422720: done 988 episodes, mean reward 65.091, eps 0.02\n",
      "Best mean reward updated 65.090 -> 65.091, model saved\n",
      "1424160: done 989 episodes, mean reward 65.108, eps 0.02\n",
      "Best mean reward updated 65.091 -> 65.108, model saved\n",
      "1425600: done 990 episodes, mean reward 65.037, eps 0.02\n",
      "1427040: done 991 episodes, mean reward 64.976, eps 0.02\n",
      "1428480: done 992 episodes, mean reward 64.996, eps 0.02\n",
      "1429920: done 993 episodes, mean reward 64.984, eps 0.02\n",
      "1431360: done 994 episodes, mean reward 64.815, eps 0.02\n",
      "1432800: done 995 episodes, mean reward 64.712, eps 0.02\n",
      "1434240: done 996 episodes, mean reward 64.621, eps 0.02\n",
      "1435680: done 997 episodes, mean reward 64.643, eps 0.02\n",
      "1437120: done 998 episodes, mean reward 64.460, eps 0.02\n",
      "1438560: done 999 episodes, mean reward 64.427, eps 0.02\n",
      "1440000: done 1000 episodes, mean reward 64.258, eps 0.02\n",
      "1441440: done 1001 episodes, mean reward 64.260, eps 0.02\n",
      "1442880: done 1002 episodes, mean reward 64.311, eps 0.02\n",
      "1444320: done 1003 episodes, mean reward 64.285, eps 0.02\n",
      "1445760: done 1004 episodes, mean reward 64.426, eps 0.02\n",
      "1447200: done 1005 episodes, mean reward 64.549, eps 0.02\n",
      "1448640: done 1006 episodes, mean reward 64.572, eps 0.02\n",
      "1450080: done 1007 episodes, mean reward 64.530, eps 0.02\n",
      "1451520: done 1008 episodes, mean reward 64.450, eps 0.02\n",
      "1452960: done 1009 episodes, mean reward 64.443, eps 0.02\n",
      "1454400: done 1010 episodes, mean reward 64.485, eps 0.02\n",
      "1455840: done 1011 episodes, mean reward 64.636, eps 0.02\n",
      "1457280: done 1012 episodes, mean reward 64.707, eps 0.02\n",
      "1458720: done 1013 episodes, mean reward 64.716, eps 0.02\n",
      "1460160: done 1014 episodes, mean reward 64.714, eps 0.02\n",
      "1461600: done 1015 episodes, mean reward 64.724, eps 0.02\n",
      "1463040: done 1016 episodes, mean reward 64.693, eps 0.02\n",
      "1464480: done 1017 episodes, mean reward 64.669, eps 0.02\n",
      "1465920: done 1018 episodes, mean reward 64.683, eps 0.02\n",
      "1467360: done 1019 episodes, mean reward 64.695, eps 0.02\n",
      "1468800: done 1020 episodes, mean reward 64.530, eps 0.02\n",
      "1470240: done 1021 episodes, mean reward 64.530, eps 0.02\n",
      "1471680: done 1022 episodes, mean reward 64.690, eps 0.02\n",
      "1473120: done 1023 episodes, mean reward 64.730, eps 0.02\n",
      "1474560: done 1024 episodes, mean reward 64.722, eps 0.02\n",
      "1476000: done 1025 episodes, mean reward 64.807, eps 0.02\n",
      "1477440: done 1026 episodes, mean reward 64.814, eps 0.02\n",
      "1478880: done 1027 episodes, mean reward 64.827, eps 0.02\n",
      "1480320: done 1028 episodes, mean reward 64.775, eps 0.02\n",
      "1481760: done 1029 episodes, mean reward 64.966, eps 0.02\n",
      "1483200: done 1030 episodes, mean reward 64.909, eps 0.02\n",
      "1484640: done 1031 episodes, mean reward 64.883, eps 0.02\n",
      "1486080: done 1032 episodes, mean reward 64.853, eps 0.02\n",
      "1487520: done 1033 episodes, mean reward 64.877, eps 0.02\n",
      "1488960: done 1034 episodes, mean reward 64.879, eps 0.02\n",
      "1490400: done 1035 episodes, mean reward 64.866, eps 0.02\n",
      "1491840: done 1036 episodes, mean reward 64.916, eps 0.02\n",
      "1493280: done 1037 episodes, mean reward 64.869, eps 0.02\n",
      "1494720: done 1038 episodes, mean reward 64.835, eps 0.02\n",
      "1496160: done 1039 episodes, mean reward 64.917, eps 0.02\n",
      "1497600: done 1040 episodes, mean reward 64.886, eps 0.02\n",
      "1499040: done 1041 episodes, mean reward 64.895, eps 0.02\n",
      "1500480: done 1042 episodes, mean reward 64.870, eps 0.02\n",
      "1501920: done 1043 episodes, mean reward 64.822, eps 0.02\n",
      "1503360: done 1044 episodes, mean reward 64.813, eps 0.02\n",
      "1504800: done 1045 episodes, mean reward 64.755, eps 0.02\n",
      "1506240: done 1046 episodes, mean reward 64.807, eps 0.02\n",
      "1507680: done 1047 episodes, mean reward 64.875, eps 0.02\n",
      "1509120: done 1048 episodes, mean reward 64.865, eps 0.02\n",
      "1510560: done 1049 episodes, mean reward 64.932, eps 0.02\n",
      "1512000: done 1050 episodes, mean reward 64.883, eps 0.02\n",
      "1513440: done 1051 episodes, mean reward 64.876, eps 0.02\n",
      "1514880: done 1052 episodes, mean reward 64.879, eps 0.02\n",
      "1516320: done 1053 episodes, mean reward 64.870, eps 0.02\n",
      "1517760: done 1054 episodes, mean reward 64.901, eps 0.02\n",
      "1519200: done 1055 episodes, mean reward 64.938, eps 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520640: done 1056 episodes, mean reward 64.885, eps 0.02\n",
      "1522080: done 1057 episodes, mean reward 64.929, eps 0.02\n",
      "1523520: done 1058 episodes, mean reward 64.899, eps 0.02\n",
      "1524960: done 1059 episodes, mean reward 64.909, eps 0.02\n",
      "1526400: done 1060 episodes, mean reward 64.802, eps 0.02\n",
      "1527840: done 1061 episodes, mean reward 64.628, eps 0.02\n",
      "1529280: done 1062 episodes, mean reward 64.998, eps 0.02\n",
      "1530720: done 1063 episodes, mean reward 65.005, eps 0.02\n",
      "1532160: done 1064 episodes, mean reward 64.963, eps 0.02\n",
      "1533600: done 1065 episodes, mean reward 64.939, eps 0.02\n",
      "1535040: done 1066 episodes, mean reward 65.045, eps 0.02\n",
      "1536480: done 1067 episodes, mean reward 65.061, eps 0.02\n",
      "1537920: done 1068 episodes, mean reward 65.052, eps 0.02\n",
      "1539360: done 1069 episodes, mean reward 64.974, eps 0.02\n",
      "1540800: done 1070 episodes, mean reward 64.930, eps 0.02\n",
      "1542240: done 1071 episodes, mean reward 64.821, eps 0.02\n",
      "1543680: done 1072 episodes, mean reward 65.010, eps 0.02\n",
      "1545120: done 1073 episodes, mean reward 64.966, eps 0.02\n",
      "1546560: done 1074 episodes, mean reward 64.801, eps 0.02\n",
      "1548000: done 1075 episodes, mean reward 64.770, eps 0.02\n",
      "1549440: done 1076 episodes, mean reward 64.823, eps 0.02\n",
      "1550880: done 1077 episodes, mean reward 64.865, eps 0.02\n",
      "1552320: done 1078 episodes, mean reward 64.726, eps 0.02\n",
      "1553760: done 1079 episodes, mean reward 64.744, eps 0.02\n",
      "1555200: done 1080 episodes, mean reward 64.744, eps 0.02\n",
      "1556640: done 1081 episodes, mean reward 64.803, eps 0.02\n",
      "1558080: done 1082 episodes, mean reward 64.768, eps 0.02\n",
      "1559520: done 1083 episodes, mean reward 64.824, eps 0.02\n",
      "1560960: done 1084 episodes, mean reward 64.833, eps 0.02\n",
      "1562400: done 1085 episodes, mean reward 64.861, eps 0.02\n",
      "1563840: done 1086 episodes, mean reward 64.886, eps 0.02\n",
      "1565280: done 1087 episodes, mean reward 64.736, eps 0.02\n",
      "1566720: done 1088 episodes, mean reward 64.736, eps 0.02\n",
      "1568160: done 1089 episodes, mean reward 64.626, eps 0.02\n",
      "1569600: done 1090 episodes, mean reward 64.639, eps 0.02\n",
      "1571040: done 1091 episodes, mean reward 64.715, eps 0.02\n",
      "1572480: done 1092 episodes, mean reward 64.641, eps 0.02\n",
      "1573920: done 1093 episodes, mean reward 64.663, eps 0.02\n",
      "1575360: done 1094 episodes, mean reward 64.738, eps 0.02\n",
      "1576800: done 1095 episodes, mean reward 64.818, eps 0.02\n",
      "1578240: done 1096 episodes, mean reward 64.889, eps 0.02\n",
      "1579680: done 1097 episodes, mean reward 64.851, eps 0.02\n",
      "1581120: done 1098 episodes, mean reward 64.887, eps 0.02\n",
      "1582560: done 1099 episodes, mean reward 64.910, eps 0.02\n",
      "1584000: done 1100 episodes, mean reward 64.921, eps 0.02\n",
      "1585440: done 1101 episodes, mean reward 64.838, eps 0.02\n",
      "1586880: done 1102 episodes, mean reward 64.735, eps 0.02\n",
      "1588320: done 1103 episodes, mean reward 64.737, eps 0.02\n",
      "1589760: done 1104 episodes, mean reward 64.649, eps 0.02\n",
      "1591200: done 1105 episodes, mean reward 64.453, eps 0.02\n",
      "1592640: done 1106 episodes, mean reward 64.455, eps 0.02\n",
      "1594080: done 1107 episodes, mean reward 64.493, eps 0.02\n",
      "1595520: done 1108 episodes, mean reward 64.483, eps 0.02\n",
      "1596960: done 1109 episodes, mean reward 64.556, eps 0.02\n",
      "1598400: done 1110 episodes, mean reward 64.539, eps 0.02\n",
      "1599840: done 1111 episodes, mean reward 64.355, eps 0.02\n",
      "1601280: done 1112 episodes, mean reward 64.328, eps 0.02\n",
      "1602720: done 1113 episodes, mean reward 64.344, eps 0.02\n",
      "1604160: done 1114 episodes, mean reward 64.357, eps 0.02\n",
      "1605600: done 1115 episodes, mean reward 64.290, eps 0.02\n",
      "1607040: done 1116 episodes, mean reward 64.245, eps 0.02\n",
      "1608480: done 1117 episodes, mean reward 64.240, eps 0.02\n",
      "1609920: done 1118 episodes, mean reward 64.307, eps 0.02\n",
      "1611360: done 1119 episodes, mean reward 64.296, eps 0.02\n",
      "1612800: done 1120 episodes, mean reward 64.266, eps 0.02\n",
      "1614240: done 1121 episodes, mean reward 64.243, eps 0.02\n",
      "1615680: done 1122 episodes, mean reward 64.081, eps 0.02\n",
      "1617120: done 1123 episodes, mean reward 64.172, eps 0.02\n",
      "1618560: done 1124 episodes, mean reward 64.161, eps 0.02\n",
      "1620000: done 1125 episodes, mean reward 64.180, eps 0.02\n",
      "1621440: done 1126 episodes, mean reward 64.146, eps 0.02\n",
      "1622880: done 1127 episodes, mean reward 64.041, eps 0.02\n",
      "1624320: done 1128 episodes, mean reward 64.154, eps 0.02\n",
      "1625760: done 1129 episodes, mean reward 63.997, eps 0.02\n",
      "1627200: done 1130 episodes, mean reward 64.030, eps 0.02\n",
      "1628640: done 1131 episodes, mean reward 64.018, eps 0.02\n",
      "1630080: done 1132 episodes, mean reward 64.089, eps 0.02\n",
      "1631520: done 1133 episodes, mean reward 64.060, eps 0.02\n",
      "1632960: done 1134 episodes, mean reward 64.109, eps 0.02\n",
      "1634400: done 1135 episodes, mean reward 64.096, eps 0.02\n",
      "1635840: done 1136 episodes, mean reward 64.013, eps 0.02\n",
      "1637280: done 1137 episodes, mean reward 64.052, eps 0.02\n",
      "1638720: done 1138 episodes, mean reward 64.100, eps 0.02\n",
      "1640160: done 1139 episodes, mean reward 64.038, eps 0.02\n",
      "1641600: done 1140 episodes, mean reward 64.054, eps 0.02\n",
      "1643040: done 1141 episodes, mean reward 64.143, eps 0.02\n",
      "1644480: done 1142 episodes, mean reward 64.247, eps 0.02\n",
      "1645920: done 1143 episodes, mean reward 64.290, eps 0.02\n",
      "1647360: done 1144 episodes, mean reward 64.307, eps 0.02\n",
      "1648800: done 1145 episodes, mean reward 64.354, eps 0.02\n",
      "1650240: done 1146 episodes, mean reward 64.316, eps 0.02\n",
      "1651680: done 1147 episodes, mean reward 64.268, eps 0.02\n",
      "1653120: done 1148 episodes, mean reward 64.176, eps 0.02\n",
      "1654560: done 1149 episodes, mean reward 64.085, eps 0.02\n",
      "1656000: done 1150 episodes, mean reward 64.064, eps 0.02\n",
      "1657440: done 1151 episodes, mean reward 64.019, eps 0.02\n",
      "1658880: done 1152 episodes, mean reward 63.967, eps 0.02\n",
      "1660320: done 1153 episodes, mean reward 64.050, eps 0.02\n",
      "1661760: done 1154 episodes, mean reward 63.959, eps 0.02\n",
      "1663200: done 1155 episodes, mean reward 63.950, eps 0.02\n",
      "1664640: done 1156 episodes, mean reward 63.939, eps 0.02\n",
      "1666080: done 1157 episodes, mean reward 63.924, eps 0.02\n",
      "1667520: done 1158 episodes, mean reward 63.956, eps 0.02\n",
      "1668960: done 1159 episodes, mean reward 63.995, eps 0.02\n",
      "1670400: done 1160 episodes, mean reward 64.006, eps 0.02\n",
      "1671840: done 1161 episodes, mean reward 64.136, eps 0.02\n",
      "1673280: done 1162 episodes, mean reward 63.870, eps 0.02\n",
      "1674720: done 1163 episodes, mean reward 63.860, eps 0.02\n",
      "1676160: done 1164 episodes, mean reward 63.792, eps 0.02\n",
      "1677600: done 1165 episodes, mean reward 63.838, eps 0.02\n",
      "1679040: done 1166 episodes, mean reward 63.839, eps 0.02\n",
      "1680480: done 1167 episodes, mean reward 63.785, eps 0.02\n",
      "1681920: done 1168 episodes, mean reward 63.830, eps 0.02\n",
      "1683360: done 1169 episodes, mean reward 63.808, eps 0.02\n",
      "1684800: done 1170 episodes, mean reward 63.839, eps 0.02\n",
      "1686240: done 1171 episodes, mean reward 63.824, eps 0.02\n",
      "1687680: done 1172 episodes, mean reward 63.704, eps 0.02\n",
      "1689120: done 1173 episodes, mean reward 63.754, eps 0.02\n",
      "1690560: done 1174 episodes, mean reward 63.826, eps 0.02\n",
      "1692000: done 1175 episodes, mean reward 63.845, eps 0.02\n",
      "1693440: done 1176 episodes, mean reward 63.855, eps 0.02\n",
      "1694880: done 1177 episodes, mean reward 63.748, eps 0.02\n",
      "1696320: done 1178 episodes, mean reward 63.859, eps 0.02\n",
      "1697760: done 1179 episodes, mean reward 63.882, eps 0.02\n",
      "1699200: done 1180 episodes, mean reward 63.846, eps 0.02\n",
      "1700640: done 1181 episodes, mean reward 63.809, eps 0.02\n",
      "1702080: done 1182 episodes, mean reward 63.853, eps 0.02\n",
      "1703520: done 1183 episodes, mean reward 63.783, eps 0.02\n",
      "1704960: done 1184 episodes, mean reward 63.830, eps 0.02\n",
      "1706400: done 1185 episodes, mean reward 63.726, eps 0.02\n",
      "1707840: done 1186 episodes, mean reward 63.734, eps 0.02\n",
      "1709280: done 1187 episodes, mean reward 63.840, eps 0.02\n",
      "1710720: done 1188 episodes, mean reward 63.895, eps 0.02\n",
      "1712160: done 1189 episodes, mean reward 63.909, eps 0.02\n",
      "1713600: done 1190 episodes, mean reward 63.917, eps 0.02\n",
      "1715040: done 1191 episodes, mean reward 63.883, eps 0.02\n",
      "1716480: done 1192 episodes, mean reward 63.921, eps 0.02\n",
      "1717920: done 1193 episodes, mean reward 64.069, eps 0.02\n",
      "1719360: done 1194 episodes, mean reward 64.009, eps 0.02\n",
      "1720800: done 1195 episodes, mean reward 64.075, eps 0.02\n",
      "1722240: done 1196 episodes, mean reward 64.110, eps 0.02\n",
      "1723680: done 1197 episodes, mean reward 64.115, eps 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1725120: done 1198 episodes, mean reward 64.149, eps 0.02\n",
      "1726560: done 1199 episodes, mean reward 64.141, eps 0.02\n",
      "1728000: done 1200 episodes, mean reward 64.214, eps 0.02\n",
      "1729440: done 1201 episodes, mean reward 64.268, eps 0.02\n",
      "1730880: done 1202 episodes, mean reward 64.341, eps 0.02\n",
      "1732320: done 1203 episodes, mean reward 64.351, eps 0.02\n",
      "1733760: done 1204 episodes, mean reward 64.394, eps 0.02\n",
      "1735200: done 1205 episodes, mean reward 64.416, eps 0.02\n",
      "1736640: done 1206 episodes, mean reward 64.364, eps 0.02\n",
      "1738080: done 1207 episodes, mean reward 64.278, eps 0.02\n",
      "1739520: done 1208 episodes, mean reward 64.308, eps 0.02\n",
      "1740960: done 1209 episodes, mean reward 64.234, eps 0.02\n",
      "1742400: done 1210 episodes, mean reward 64.263, eps 0.02\n",
      "1743840: done 1211 episodes, mean reward 64.288, eps 0.02\n",
      "1745280: done 1212 episodes, mean reward 64.268, eps 0.02\n",
      "1746720: done 1213 episodes, mean reward 64.352, eps 0.02\n",
      "1748160: done 1214 episodes, mean reward 64.370, eps 0.02\n",
      "1749600: done 1215 episodes, mean reward 64.425, eps 0.02\n",
      "1751040: done 1216 episodes, mean reward 64.501, eps 0.02\n",
      "1752480: done 1217 episodes, mean reward 64.516, eps 0.02\n",
      "1753920: done 1218 episodes, mean reward 64.450, eps 0.02\n",
      "1755360: done 1219 episodes, mean reward 64.460, eps 0.02\n",
      "1756800: done 1220 episodes, mean reward 64.528, eps 0.02\n",
      "1758240: done 1221 episodes, mean reward 64.526, eps 0.02\n",
      "1759680: done 1222 episodes, mean reward 64.546, eps 0.02\n",
      "1761120: done 1223 episodes, mean reward 64.461, eps 0.02\n",
      "1762560: done 1224 episodes, mean reward 64.476, eps 0.02\n",
      "1764000: done 1225 episodes, mean reward 64.439, eps 0.02\n",
      "1765440: done 1226 episodes, mean reward 64.443, eps 0.02\n",
      "1766880: done 1227 episodes, mean reward 64.475, eps 0.02\n",
      "1768320: done 1228 episodes, mean reward 64.396, eps 0.02\n",
      "1769760: done 1229 episodes, mean reward 64.422, eps 0.02\n",
      "1771200: done 1230 episodes, mean reward 64.378, eps 0.02\n",
      "1772640: done 1231 episodes, mean reward 64.512, eps 0.02\n",
      "1774080: done 1232 episodes, mean reward 64.460, eps 0.02\n",
      "1775520: done 1233 episodes, mean reward 64.462, eps 0.02\n",
      "1776960: done 1234 episodes, mean reward 64.409, eps 0.02\n",
      "1778400: done 1235 episodes, mean reward 64.436, eps 0.02\n",
      "1779840: done 1236 episodes, mean reward 64.399, eps 0.02\n",
      "1781280: done 1237 episodes, mean reward 64.476, eps 0.02\n",
      "1782720: done 1238 episodes, mean reward 64.445, eps 0.02\n",
      "1784160: done 1239 episodes, mean reward 64.613, eps 0.02\n",
      "1785600: done 1240 episodes, mean reward 64.626, eps 0.02\n",
      "1787040: done 1241 episodes, mean reward 64.598, eps 0.02\n",
      "1788480: done 1242 episodes, mean reward 64.631, eps 0.02\n",
      "1789920: done 1243 episodes, mean reward 64.714, eps 0.02\n",
      "1791360: done 1244 episodes, mean reward 64.659, eps 0.02\n",
      "1792800: done 1245 episodes, mean reward 64.677, eps 0.02\n",
      "1794240: done 1246 episodes, mean reward 64.700, eps 0.02\n",
      "1795680: done 1247 episodes, mean reward 64.732, eps 0.02\n",
      "1797120: done 1248 episodes, mean reward 64.753, eps 0.02\n",
      "1798560: done 1249 episodes, mean reward 64.769, eps 0.02\n",
      "1800000: done 1250 episodes, mean reward 64.848, eps 0.02\n",
      "1801440: done 1251 episodes, mean reward 65.032, eps 0.02\n",
      "1802880: done 1252 episodes, mean reward 65.248, eps 0.02\n",
      "Best mean reward updated 65.108 -> 65.248, model saved\n",
      "1804320: done 1253 episodes, mean reward 65.173, eps 0.02\n",
      "1805760: done 1254 episodes, mean reward 65.223, eps 0.02\n",
      "1807200: done 1255 episodes, mean reward 65.209, eps 0.02\n",
      "1808640: done 1256 episodes, mean reward 65.210, eps 0.02\n",
      "1810080: done 1257 episodes, mean reward 65.162, eps 0.02\n",
      "1811520: done 1258 episodes, mean reward 65.097, eps 0.02\n",
      "1812960: done 1259 episodes, mean reward 65.099, eps 0.02\n",
      "1814400: done 1260 episodes, mean reward 65.091, eps 0.02\n",
      "1815840: done 1261 episodes, mean reward 65.051, eps 0.02\n",
      "1817280: done 1262 episodes, mean reward 65.032, eps 0.02\n",
      "1818720: done 1263 episodes, mean reward 64.957, eps 0.02\n",
      "1820160: done 1264 episodes, mean reward 65.007, eps 0.02\n",
      "1821600: done 1265 episodes, mean reward 64.948, eps 0.02\n",
      "1823040: done 1266 episodes, mean reward 64.924, eps 0.02\n",
      "1824480: done 1267 episodes, mean reward 64.923, eps 0.02\n",
      "1825920: done 1268 episodes, mean reward 64.823, eps 0.02\n",
      "1827360: done 1269 episodes, mean reward 64.798, eps 0.02\n",
      "1828800: done 1270 episodes, mean reward 64.799, eps 0.02\n",
      "1830240: done 1271 episodes, mean reward 64.766, eps 0.02\n",
      "1831680: done 1272 episodes, mean reward 64.814, eps 0.02\n",
      "1833120: done 1273 episodes, mean reward 64.765, eps 0.02\n",
      "1834560: done 1274 episodes, mean reward 64.725, eps 0.02\n",
      "1836000: done 1275 episodes, mean reward 64.749, eps 0.02\n",
      "1837440: done 1276 episodes, mean reward 64.707, eps 0.02\n",
      "1838880: done 1277 episodes, mean reward 64.816, eps 0.02\n",
      "1840320: done 1278 episodes, mean reward 64.723, eps 0.02\n",
      "1841760: done 1279 episodes, mean reward 64.832, eps 0.02\n",
      "1843200: done 1280 episodes, mean reward 64.855, eps 0.02\n",
      "1844640: done 1281 episodes, mean reward 64.883, eps 0.02\n",
      "1846080: done 1282 episodes, mean reward 64.832, eps 0.02\n",
      "1847520: done 1283 episodes, mean reward 64.860, eps 0.02\n",
      "1848960: done 1284 episodes, mean reward 64.831, eps 0.02\n",
      "1850400: done 1285 episodes, mean reward 64.863, eps 0.02\n",
      "1851840: done 1286 episodes, mean reward 64.880, eps 0.02\n",
      "1853280: done 1287 episodes, mean reward 64.865, eps 0.02\n",
      "1854720: done 1288 episodes, mean reward 64.841, eps 0.02\n",
      "1856160: done 1289 episodes, mean reward 64.916, eps 0.02\n",
      "1857600: done 1290 episodes, mean reward 64.888, eps 0.02\n",
      "1859040: done 1291 episodes, mean reward 64.869, eps 0.02\n",
      "1860480: done 1292 episodes, mean reward 64.953, eps 0.02\n",
      "1861920: done 1293 episodes, mean reward 64.808, eps 0.02\n",
      "1863360: done 1294 episodes, mean reward 64.808, eps 0.02\n",
      "1864800: done 1295 episodes, mean reward 64.756, eps 0.02\n",
      "1866240: done 1296 episodes, mean reward 64.744, eps 0.02\n",
      "1867680: done 1297 episodes, mean reward 64.700, eps 0.02\n",
      "1869120: done 1298 episodes, mean reward 64.637, eps 0.02\n",
      "1870560: done 1299 episodes, mean reward 64.585, eps 0.02\n",
      "1872000: done 1300 episodes, mean reward 64.564, eps 0.02\n",
      "1873440: done 1301 episodes, mean reward 64.607, eps 0.02\n",
      "1874880: done 1302 episodes, mean reward 64.626, eps 0.02\n",
      "1876320: done 1303 episodes, mean reward 64.631, eps 0.02\n",
      "1877760: done 1304 episodes, mean reward 64.618, eps 0.02\n",
      "1879200: done 1305 episodes, mean reward 64.524, eps 0.02\n",
      "1880640: done 1306 episodes, mean reward 64.534, eps 0.02\n",
      "1882080: done 1307 episodes, mean reward 64.564, eps 0.02\n",
      "1883520: done 1308 episodes, mean reward 64.520, eps 0.02\n",
      "1884960: done 1309 episodes, mean reward 64.508, eps 0.02\n",
      "1886400: done 1310 episodes, mean reward 64.443, eps 0.02\n",
      "1887840: done 1311 episodes, mean reward 64.436, eps 0.02\n",
      "1889280: done 1312 episodes, mean reward 64.468, eps 0.02\n",
      "1890720: done 1313 episodes, mean reward 64.404, eps 0.02\n",
      "1892160: done 1314 episodes, mean reward 64.360, eps 0.02\n",
      "1893600: done 1315 episodes, mean reward 64.394, eps 0.02\n",
      "1895040: done 1316 episodes, mean reward 64.356, eps 0.02\n",
      "1896480: done 1317 episodes, mean reward 64.354, eps 0.02\n",
      "1897920: done 1318 episodes, mean reward 64.384, eps 0.02\n",
      "1899360: done 1319 episodes, mean reward 64.287, eps 0.02\n",
      "1900800: done 1320 episodes, mean reward 64.182, eps 0.02\n",
      "1902240: done 1321 episodes, mean reward 64.231, eps 0.02\n",
      "1903680: done 1322 episodes, mean reward 64.197, eps 0.02\n",
      "1905120: done 1323 episodes, mean reward 64.162, eps 0.02\n",
      "1906560: done 1324 episodes, mean reward 64.141, eps 0.02\n",
      "1908000: done 1325 episodes, mean reward 64.085, eps 0.02\n",
      "1909440: done 1326 episodes, mean reward 64.059, eps 0.02\n",
      "1910880: done 1327 episodes, mean reward 64.018, eps 0.02\n",
      "1912320: done 1328 episodes, mean reward 64.046, eps 0.02\n",
      "1913760: done 1329 episodes, mean reward 64.302, eps 0.02\n",
      "1915200: done 1330 episodes, mean reward 64.369, eps 0.02\n",
      "1916640: done 1331 episodes, mean reward 64.238, eps 0.02\n",
      "1918080: done 1332 episodes, mean reward 64.244, eps 0.02\n",
      "1919520: done 1333 episodes, mean reward 64.268, eps 0.02\n",
      "1920960: done 1334 episodes, mean reward 64.314, eps 0.02\n",
      "1922400: done 1335 episodes, mean reward 64.300, eps 0.02\n",
      "1923840: done 1336 episodes, mean reward 64.383, eps 0.02\n",
      "1925280: done 1337 episodes, mean reward 64.334, eps 0.02\n",
      "1926720: done 1338 episodes, mean reward 64.290, eps 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1928160: done 1339 episodes, mean reward 64.119, eps 0.02\n",
      "1929600: done 1340 episodes, mean reward 64.200, eps 0.02\n",
      "1931040: done 1341 episodes, mean reward 64.253, eps 0.02\n",
      "1932480: done 1342 episodes, mean reward 64.243, eps 0.02\n",
      "1933920: done 1343 episodes, mean reward 64.204, eps 0.02\n",
      "1935360: done 1344 episodes, mean reward 64.184, eps 0.02\n",
      "1936800: done 1345 episodes, mean reward 64.078, eps 0.02\n",
      "1938240: done 1346 episodes, mean reward 64.036, eps 0.02\n",
      "1939680: done 1347 episodes, mean reward 64.032, eps 0.02\n",
      "1941120: done 1348 episodes, mean reward 64.040, eps 0.02\n",
      "1942560: done 1349 episodes, mean reward 64.032, eps 0.02\n",
      "1944000: done 1350 episodes, mean reward 63.979, eps 0.02\n",
      "1945440: done 1351 episodes, mean reward 63.909, eps 0.02\n",
      "1946880: done 1352 episodes, mean reward 63.733, eps 0.02\n",
      "1948320: done 1353 episodes, mean reward 63.707, eps 0.02\n",
      "1949760: done 1354 episodes, mean reward 63.746, eps 0.02\n",
      "1951200: done 1355 episodes, mean reward 63.741, eps 0.02\n",
      "1952640: done 1356 episodes, mean reward 63.746, eps 0.02\n",
      "1954080: done 1357 episodes, mean reward 63.808, eps 0.02\n",
      "1955520: done 1358 episodes, mean reward 64.029, eps 0.02\n",
      "1956960: done 1359 episodes, mean reward 63.974, eps 0.02\n",
      "1958400: done 1360 episodes, mean reward 63.985, eps 0.02\n",
      "1959840: done 1361 episodes, mean reward 64.003, eps 0.02\n",
      "1961280: done 1362 episodes, mean reward 64.042, eps 0.02\n",
      "1962720: done 1363 episodes, mean reward 64.069, eps 0.02\n",
      "1964160: done 1364 episodes, mean reward 64.103, eps 0.02\n",
      "1965600: done 1365 episodes, mean reward 64.109, eps 0.02\n",
      "1967040: done 1366 episodes, mean reward 64.114, eps 0.02\n",
      "1968480: done 1367 episodes, mean reward 64.147, eps 0.02\n",
      "1969920: done 1368 episodes, mean reward 64.287, eps 0.02\n",
      "1971360: done 1369 episodes, mean reward 64.388, eps 0.02\n",
      "1972800: done 1370 episodes, mean reward 64.497, eps 0.02\n",
      "1974240: done 1371 episodes, mean reward 64.550, eps 0.02\n",
      "1975680: done 1372 episodes, mean reward 64.598, eps 0.02\n",
      "1977120: done 1373 episodes, mean reward 64.702, eps 0.02\n",
      "1978560: done 1374 episodes, mean reward 64.661, eps 0.02\n",
      "1980000: done 1375 episodes, mean reward 64.648, eps 0.02\n",
      "1981440: done 1376 episodes, mean reward 64.649, eps 0.02\n",
      "1982880: done 1377 episodes, mean reward 64.570, eps 0.02\n",
      "1984320: done 1378 episodes, mean reward 64.570, eps 0.02\n",
      "1985760: done 1379 episodes, mean reward 64.548, eps 0.02\n",
      "1987200: done 1380 episodes, mean reward 64.538, eps 0.02\n",
      "1988640: done 1381 episodes, mean reward 64.564, eps 0.02\n",
      "1990080: done 1382 episodes, mean reward 64.554, eps 0.02\n",
      "1991520: done 1383 episodes, mean reward 64.598, eps 0.02\n",
      "1992960: done 1384 episodes, mean reward 64.565, eps 0.02\n",
      "1994400: done 1385 episodes, mean reward 64.531, eps 0.02\n",
      "1995840: done 1386 episodes, mean reward 64.506, eps 0.02\n",
      "1997280: done 1387 episodes, mean reward 64.521, eps 0.02\n",
      "1998720: done 1388 episodes, mean reward 64.473, eps 0.02\n",
      "2000160: done 1389 episodes, mean reward 64.425, eps 0.02\n",
      "2001600: done 1390 episodes, mean reward 64.455, eps 0.02\n",
      "2003040: done 1391 episodes, mean reward 64.520, eps 0.02\n",
      "2004480: done 1392 episodes, mean reward 64.393, eps 0.02\n",
      "2005920: done 1393 episodes, mean reward 64.520, eps 0.02\n",
      "2007360: done 1394 episodes, mean reward 64.536, eps 0.02\n",
      "2008800: done 1395 episodes, mean reward 64.592, eps 0.02\n",
      "2010240: done 1396 episodes, mean reward 64.654, eps 0.02\n",
      "2011680: done 1397 episodes, mean reward 64.677, eps 0.02\n",
      "2013120: done 1398 episodes, mean reward 64.681, eps 0.02\n",
      "2014560: done 1399 episodes, mean reward 64.725, eps 0.02\n",
      "2016000: done 1400 episodes, mean reward 64.702, eps 0.02\n",
      "2017440: done 1401 episodes, mean reward 64.701, eps 0.02\n",
      "2018880: done 1402 episodes, mean reward 64.686, eps 0.02\n",
      "2020320: done 1403 episodes, mean reward 64.624, eps 0.02\n",
      "2021760: done 1404 episodes, mean reward 64.594, eps 0.02\n",
      "2023200: done 1405 episodes, mean reward 64.773, eps 0.02\n",
      "2024640: done 1406 episodes, mean reward 64.795, eps 0.02\n",
      "2026080: done 1407 episodes, mean reward 64.856, eps 0.02\n",
      "2027520: done 1408 episodes, mean reward 64.901, eps 0.02\n",
      "2028960: done 1409 episodes, mean reward 64.885, eps 0.02\n",
      "2030400: done 1410 episodes, mean reward 64.931, eps 0.02\n",
      "2031840: done 1411 episodes, mean reward 64.916, eps 0.02\n",
      "2033280: done 1412 episodes, mean reward 64.888, eps 0.02\n",
      "2034720: done 1413 episodes, mean reward 64.911, eps 0.02\n",
      "2036160: done 1414 episodes, mean reward 64.906, eps 0.02\n",
      "2037600: done 1415 episodes, mean reward 64.921, eps 0.02\n",
      "2039040: done 1416 episodes, mean reward 64.936, eps 0.02\n",
      "2040480: done 1417 episodes, mean reward 64.966, eps 0.02\n",
      "2041920: done 1418 episodes, mean reward 64.969, eps 0.02\n",
      "2043360: done 1419 episodes, mean reward 65.025, eps 0.02\n",
      "2044800: done 1420 episodes, mean reward 65.114, eps 0.02\n",
      "2046240: done 1421 episodes, mean reward 65.220, eps 0.02\n",
      "2047680: done 1422 episodes, mean reward 65.250, eps 0.02\n",
      "Best mean reward updated 65.248 -> 65.250, model saved\n",
      "2049120: done 1423 episodes, mean reward 65.247, eps 0.02\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "env = TradingSession(action_space_config = 'discrete')\n",
    "\n",
    "net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "tgt_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "\n",
    "print(net)\n",
    "\n",
    "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "agent = Agent(env, buffer)\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "step_idx = 0\n",
    "best_mean_reward = None\n",
    "\n",
    "while True:\n",
    "    step_idx += 1\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START - step_idx / EPSILON_DECAY)    \n",
    "    \n",
    "    reward = agent.play_step(net, epsilon, device=device)\n",
    "    \n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "        print(\"%d: done %d episodes, mean reward %.3f, eps %.2f\" % (step_idx, len(total_rewards), mean_reward, epsilon))\n",
    "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "            torch.save(net.state_dict(), \"model.dat\")\n",
    "            if best_mean_reward is not None:\n",
    "                print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
    "            best_mean_reward = mean_reward\n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print(\"Solved in %d steps!\" % step_idx)\n",
    "            break\n",
    "\n",
    "    if len(buffer) < REPLAY_START_SIZE:\n",
    "        continue\n",
    "\n",
    "    if step_idx % SYNC_TARGET_STEPS == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device=device, cuda_async = True)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
